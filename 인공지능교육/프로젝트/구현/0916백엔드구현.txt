from __future__ import division
import re
import sys

from google.cloud import speech

import pyaudio
from six.moves import queue
import wave
from google.cloud import texttospeech # pip install --upgrade google-cloud-texttospeech
from playsound import playsound # 음성파일 재생하는 라이브러리 : pip install playsound

import requests    # pip install requests
from datetime import datetime
import time
import json

import os

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/test/gcloud_key.json"

from google.cloud import speech
client = speech.SpeechClient()
gcs_uri = "gs://cloud-samples-data/speech/brooklyn_bridge.raw"
audio = speech.RecognitionAudio(uri=gcs_uri)
config = speech.RecognitionConfig(
    encoding = speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=16000,
    language_code = 'en-US',
)
response = client.recognize(config = config, audio = audio)
for result in response.results :
    print('Transcript: {}'.format(result.alternatives[0].transcript))


# 녹음 파일에서 음성 인식 진행
'''
def transcribe_file(speech_file):
    """Transcribe the given audio file."""
    from google.cloud import speech
    import io

    client = speech.SpeechClient()

    with io.open(speech_file, "rb") as audio_file:
        content = audio_file.read()

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="ko-KR",
    )

    response = client.recognize(config=config, audio=audio)

    # Each result is for a consecutive portion of the audio. Iterate through
    # them to get the transcripts for the entire audio file.
    for result in response.results:
        # The first alternative is the most likely one for this portion.
        print(u"Transcript: {}".format(result.alternatives[0].transcript))


transcribe_file('C:/Users/slinfo/PycharmProjects/recoder/file1.wav')
'''

# 실시간 오디오 스트리밍에서 스트리밍 음성 인식 수행하기





# Audio recording parameters
RATE = 16000
CHUNK = int(RATE / 10)  # 100ms

class MicrophoneStream(object):
    """Opens a recording stream as a generator yielding the audio chunks."""

    def __init__(self, rate, chunk):
        self._rate = rate
        self._chunk = chunk

        # Create a thread-safe buffer of audio data
        self._buff = queue.Queue()
        self.closed = True

    def __enter__(self):
        self._audio_interface = pyaudio.PyAudio()
        self._audio_stream = self._audio_interface.open(
            format=pyaudio.paInt16,
            # The API currently only supports 1-channel (mono) audio
            # https://goo.gl/z757pE
            channels=1,
            rate=self._rate,
            input=True,
            frames_per_buffer=self._chunk,
            # Run the audio stream asynchronously to fill the buffer object.
            # This is necessary so that the input device's buffer doesn't
            # overflow while the calling thread makes network requests, etc.
            stream_callback=self._fill_buffer,
        )

        self.closed = False

        return self

    def __exit__(self, type, value, traceback):
        self._audio_stream.stop_stream()
        self._audio_stream.close()
        self.closed = True
        # Signal the generator to terminate so that the client's
        # streaming_recognize method will not block the process termination.
        self._buff.put(None)
        self._audio_interface.terminate()

    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):
        """Continuously collect data from the audio stream, into the buffer."""
        self._buff.put(in_data)
        return None, pyaudio.paContinue

    def generator(self):
        while not self.closed:
            # Use a blocking get() to ensure there's at least one chunk of
            # data, and stop iteration if the chunk is None, indicating the
            # end of the audio stream.
            chunk = self._buff.get()
            if chunk is None:
                return
            data = [chunk]

            # Now consume whatever other data's still buffered.
            while True:
                try:
                    chunk = self._buff.get(block=False)
                    if chunk is None:
                        return
                    data.append(chunk)
                except queue.Empty:
                    break

            yield b"".join(data)

def listen_print_loop(responses):

    num_chars_printed = 0
    for response in responses:
        if not response.results:
            continue

        # The `results` list is consecutive. For streaming, we only care about
        # the first result being considered, since once it's `is_final`, it
        # moves on to considering the next utterance.
        result = response.results[0]
        if not result.alternatives:
            continue

        # Display the transcription of the top alternative.
        transcript = result.alternatives[0].transcript

        # Display interim results, but with a carriage return at the end of the
        # line, so subsequent lines will overwrite them.
        #
        # If the previous result was longer than this one, we need to print
        # some extra spaces to overwrite the previous result
        overwrite_chars = " " * (num_chars_printed - len(transcript))

        if not result.is_final:
            sys.stdout.write(transcript + overwrite_chars + "\r")
            sys.stdout.flush()

            num_chars_printed = len(transcript)

        else:
            print(transcript + overwrite_chars)
            word = transcript + overwrite_chars

            # Exit recognition if any of the transcribed phrases could be
            # one of our keywords.
            if re.search(r"\b(종료|끄기)\b", transcript, re.I):
                print("Exiting..")
                break

            if re.search(r"\b(시작)\b", transcript, re.I):
                # ai 호출하면 반응
                print("말씀하세요")

                print(translate(word))

                TextToSpeech('네? 말씀하세요.')
                playsound('output.mp3', True)  # C:\\Users\\slinfo\\PycharmProjects\\0901_gcloud\\파일.확장자 로 지정해줘야 하는 경우도 있음
                # 반응 후 내 말을 녹음파일로 저장


            num_chars_printed = 0

def main():
    # See http://g.co/cloud/speech/docs/languages
    # for a list of supported languages.
    language_code = "ko-KR"  # a BCP-47 language tag

    client = speech.SpeechClient()
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=RATE,
        language_code=language_code,
    )

    streaming_config = speech.StreamingRecognitionConfig(
        config=config, interim_results=True
    )

    with MicrophoneStream(RATE, CHUNK) as stream:
        audio_generator = stream.generator()
        requests = (
            speech.StreamingRecognizeRequest(audio_content=content)
            for content in audio_generator
        )

        responses = client.streaming_recognize(streaming_config, requests)

        # Now, put the transcription responses to use.
        listen_print_loop(responses)







'''
def recoder() :             #녹음해서 파일로 저장
    FORMAT = pyaudio.paInt16
    CHANNELS = 1
    RATE = 16000
    CHUNK = 1024
    RECORD_SECONDS = 5
    WAVE_OUTPUT_FILENAME = 'file.wav'
    audio = pyaudio.PyAudio()

    stream = audio.open(format=FORMAT, channels=CHANNELS,
                        rate=RATE, input=True,
                        frames_per_buffer=CHUNK)
    print('recording,,,')
    frames = []

    for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):
        data = stream.read(CHUNK)
        frames.append(data)
    print('finished recording')

    stream.stop_stream()
    stream.close()
    audio.terminate()

    waveFile = wave.open(WAVE_OUTPUT_FILENAME, 'wb')
    waveFile.setnchannels(CHANNELS)
    waveFile.setsampwidth(audio.get_sample_size(FORMAT))
    waveFile.setframerate(RATE)
    waveFile.writeframes(b''.join(frames))
    waveFile.close()
'''


'''
def transcribe_file(speech_file):            # 녹음파일을 음성인식해서 출력
    """Transcribe the given audio file."""
    from google.cloud import speech
    import io

    client = speech.SpeechClient()

    with io.open(speech_file, "rb") as audio_file:
        content = audio_file.read()

    audio = speech.RecognitionAudio(content=content)
    config = speech.RecognitionConfig(
        encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
        sample_rate_hertz=16000,
        language_code="ko-KR",
    )

    response = client.recognize(config=config, audio=audio)

    # Each result is for a consecutive portion of the audio. Iterate through
    # them to get the transcripts for the entire audio file.
    for result in response.results:
        # The first alternative is the most likely one for this portion.
        print(u"Transcript: {}".format(result.alternatives[0].transcript))

        if '날씨' in result.alternatives[0].transcript :
            TextToSpeech(str(weather_crawling()))
            time.sleep(1)                      # playsound가 너무 빨리 실행되면 안돼서 1초 지연
            playsound('output.mp3', True)    # 파일경로로 지정해줘야하는 경우도 있음

        if '시간' in result.alternatives[0].transcript :
            TextToSpeech(str(timenow()))
            time.sleep(1)                    # playsound가 너무 빨리 실행되면 안돼서 1초 지연
            playsound('output.mp3', True)
'''

# 텍스트 입력하면 음성파일로 저장하는 함수 만들기
def TextToSpeech(text) :

    client = texttospeech.TextToSpeechClient()

    # 합성할 text 지정
    synthesis_input = texttospeech.SynthesisInput(text=text)

    # voice gender ("neutral")
    voice = texttospeech.VoiceSelectionParams(
        language_code="ko-KR",
        name = 'ko-KR-Wavenet-D',
        ssml_gender=texttospeech.SsmlVoiceGender.MALE
    )

    # Select the type of audio file you want returned
    audio_config = texttospeech.AudioConfig(
        audio_encoding=texttospeech.AudioEncoding.MP3
    )

    # Perform the text-to-speech request on the text input with the selected
    # voice parameters and audio file type
    response = client.synthesize_speech(
        input=synthesis_input, voice=voice, audio_config=audio_config
    )

    # The response's audio_content is binary.
    with open("c:\\test\\output.mp3", "wb") as out:
        # Write the response to the output file.
        out.write(response.audio_content)
        print('Audio content written to file "output.mp3"')


"""

# 기상예보 크롤링
def weather_crawling():
    key = 'GtbNVjtp2jvReVCmZ3V%2F4mPHVV02KkS3%2B86s%2BApiM4dnx8KoVGUWqmRZj7rH%2FJ96%2Ff9ZkBPMvTBTjNGuLoLaGA%3D%3D'
    url = 'http://apis.data.go.kr/1360000/VilageFcstInfoService/getUltraSrtFcst?serviceKey=' + key + '&numOfRows=100&pageNo=1&base_date=' + datetime.today().strftime("%Y%m%d") + '&base_time=' + str(int(datetime.today().strftime("%H%M"))-100) + '&nx=62&ny=122&dataType=JSON'

    req = requests.get(url)
    html = req.content

    result = json.loads(html)

    for i in result['response']['body']['items']['item']:
        if i['category'] == 'T1H':
            print('현재 기온은 ', i['fcstValue'], '도 입니다.')
            return '현재 기온은 ', i['fcstValue'], '도 입니다.'
        '''
    for i in result['response']['body']['items']['item']:
        if i['category'] == 'POP':
            print('현재 강수 확률은 ', i['fcstValue'], '퍼센트 입니다.')
            '''
# 시간알려주는 함수

def timenow() :
    time = datetime.today().strftime("%H%M")
    if int(time[:2]) < 12 :
        return '현재 시간은 오전 ',time[:2],'시 ',time[2:],'분 입니다.'
    else :
        return '현재 시간은 오후 ',time[:2],'시 ',time[2:],'분 입니다.'
"""





#--------------------------------------------------------------------------------------------------------

import requests, json

def translate(text) :
    # URL 지정
    url = 'http://svc.saltlux.ai:31781'

    # Header 정보 지정
    headers = {'Content-Type': 'application/json; charset=utf-8'}

    # Request Parameter 정보 지정
    params = {
        "text": text,
        "key": "ffe069c1-6aa8-4652-bb43-a22ee1a8bc2c",
        "serviceId": "00294481085"
    }

    response = requests.post(url, headers=headers, data=json.dumps(params))

    return response.content








if __name__ == "__main__":
    main()



