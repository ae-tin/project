{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jDe-m_5aq4"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH9oF2RleEYl"
   },
   "source": [
    "## Load Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XamZpd5EeGHR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import session_info\n",
    "import pdb\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w9lA5-iVI0d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import pickle as pk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl2lXOAv9ljx"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D0pjVLfcfMl7"
   },
   "outputs": [],
   "source": [
    "RUNTIME = 'none'\n",
    "ARGS = {\n",
    "  'model_id' : 'm20220727e',\n",
    "  'global_seed' : 123,\n",
    "  'shuffle_size' : 1000,\n",
    "  'max_width' : 100,\n",
    "  'head_len' : 17,\n",
    "  'tail_len' : 13,\n",
    "  'pct_ds' : 1, # % of total data for training/testing,\n",
    "  'train_split' : 0.95,\n",
    "  'alphabets' : {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3, 'N' : 4, 'M' : 5},\n",
    "  'initial_lr' : 1e-15,\n",
    "  'max_lr' : 3e-4,\n",
    "  'initial_epoch': 0,\n",
    "  'epochs' : 20,\n",
    "  'batch_size' : 512,\n",
    "  'dropout_rate' : 0.1,\n",
    "  'kmer': 10,\n",
    "  'strides' : 1,\n",
    "  'embedding_dim' : 512,\n",
    "  'num_heads' : 8,\n",
    "  'ff_mult' : 4,\n",
    "  'num_projectors' : 32,\n",
    "  'n_blocks_regressor' : 4,\n",
    "  'warmup_steps' : 12500, # ~ 1 epoch\n",
    "  'mask_ratio' : 0.05,\n",
    "  'remote_sample_submission_file' : 'https://raw.githubusercontent.com/de-Boer-Lab/DREAM-2022/main/sample_submission.json',\n",
    "  'eval' : False,\n",
    "  'device':'cuda:1'\n",
    "}\n",
    "if RUNTIME == 'msi':\n",
    "  ARGS['remote_data_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/training_data/pct_ds=' + str(ARGS['pct_ds']) + '/'\n",
    "  ARGS['local_data_dir'] = re.sub('https://', './', ARGS['remote_data_dir'])\n",
    "  ARGS['remote_checkpoint_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/notebooks_msi/' + ARGS['model_id'] + '/tf_ckpts/'\n",
    "  ARGS['remote_log_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/notebooks_msi/' + ARGS['model_id'] + '/log/'\n",
    "  ARGS['local_checkpoint_dir'] = re.sub('https://', './', ARGS['remote_checkpoint_dir'])\n",
    "  ARGS['local_log_dir'] = re.sub('https://', './', ARGS['remote_log_dir'])\n",
    "  ARGS['remote_test_data'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/test_sequences.txt.gz'\n",
    "  ARGS['local_test_data'] = re.sub('https://', './', ARGS['remote_test_data'])\n",
    "  ARGS['local_sample_submission_file'] = re.sub('https://', './', ARGS['remote_sample_submission_file'])\n",
    "  ARGS['remote_prediction_file'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/predictions/' + ARGS['model_id'] + '/pred.json'\n",
    "  ARGS['local_prediction_file'] = re.sub('https://', './', ARGS['remote_prediction_file'])\n",
    "  ARGS['s3_prediction_file'] = re.sub('https://s3.msi.umn.edu', 's3://', ARGS['remote_prediction_file'])\n",
    "  ARGS['remote_prediction_tsv_file'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/predictions/' + ARGS['model_id'] + '/pred.tsv'\n",
    "  ARGS['local_prediction_tsv_file'] = re.sub('https://', './', ARGS['remote_prediction_tsv_file'])\n",
    "  ARGS['s3_prediction_tsv_file'] = re.sub('https://s3.msi.umn.edu', 's3://', ARGS['remote_prediction_tsv_file'])\n",
    "else:\n",
    "  ARGS['local_data_dir'] = '/content/drive/MyDrive/training_data/pct_ds=' + str(ARGS['pct_ds']) + '/'\n",
    "  ARGS['local_checkpoint_dir'] = '/content/drive/MyDrive/' + ARGS['model_id'] + '/tf_ckpts/'\n",
    "  ARGS['local_log_dir'] = '/content/drive/MyDrive/' + ARGS['model_id'] + '/log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS['local_data_dir'] = '/Data1/PGE/torch_ti/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZhZvwWifIpzY"
   },
   "outputs": [],
   "source": [
    "with open(ARGS['local_data_dir']+\"data.pk\",\"rb\") as fr:\n",
    "    data = pk.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i5XrE1H2Iwq"
   },
   "source": [
    "### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5LTFxNSIWQID"
   },
   "outputs": [],
   "source": [
    "np.random.seed(ARGS['global_seed'])\n",
    "torch.manual_seed(ARGS['global_seed'])\n",
    "python_random.seed(ARGS['global_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZLHuW06Tcm"
   },
   "source": [
    "### pearson_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bDou1j1A6S2j"
   },
   "outputs": [],
   "source": [
    "def pearson_r(x, y):\n",
    "    x = torch.tensor(x,dtype=torch.float32)\n",
    "    y = torch.tensor(y,dtype=torch.float32)\n",
    "    mx = torch.mean(x, axis = 0, keepdims = True)\n",
    "    my = torch.mean(y, axis = 0, keepdims = True)\n",
    "    xm = x - mx\n",
    "    ym = y - my\n",
    "    t1_norm = F.normalize(xm, p=2, dim=0)\n",
    "    t2_norm = F.normalize(ym, p=2, dim=0)\n",
    "    return torch.sum(torch.mul(t1_norm, t2_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E4kvC_v6oYG",
    "outputId": "53ce49be-7138-490b-ba8f-85a92dc3592e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson r (stats.pearsonr): -0.09270195576139686\n",
      "pearson r (pearson_r): -0.09270194172859192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10275/2976460682.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x,dtype=torch.float32)\n",
      "/tmp/ipykernel_10275/2976460682.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "print('pearson r (stats.pearsonr): {}'.format(stats.pearsonr(x, y)[0]))\n",
    "print('pearson r (pearson_r): {}'.format(pearson_r(torch.unsqueeze(torch.Tensor(x),1), torch.unsqueeze(torch.Tensor(y),1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrQ9T4-oSk8X"
   },
   "source": [
    "### GLULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oMX5LC1aSkRK"
   },
   "outputs": [],
   "source": [
    "class GLULayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(GLULayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out,gate = torch.chunk(x, 2, dim = self.dim)\n",
    "        return out * self.sig(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivpHjDlQSxUU",
    "outputId": "6f85a037-81fc-4924-cbb0-074aa9e0d808"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = GLULayer(dim = 1)\n",
    "x = torch.randn(3,6,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHdNKhzVvsMf"
   },
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WeSyNETvvuex"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SwiGLULayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SwiGLULayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.swish = nn.SiLU() # same as swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, gate = torch.chunk(x, 2, dim = self.dim)\n",
    "        return out * self.swish(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuwI-YXjvuex",
    "outputId": "7e3be8c3-6e7a-4886-d85d-26c9c763b953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = SwiGLULayer(dim = 1)\n",
    "x = torch.randn(3,6,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7BVgkw-w4UO"
   },
   "source": [
    "### FeedForwardSwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eVdwwIxbw4UP"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardSwiGLU(nn.Module):\n",
    "    def __init__(self, embedding_dim, mult=4, rate = 0.0, use_bias = False):\n",
    "        super(FeedForwardSwiGLU, self).__init__()\n",
    "        swiglu_out = int(embedding_dim * mult/2)\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim,eps = 1e-6)\n",
    "        self.linear1 = nn.Linear(embedding_dim,embedding_dim * mult, bias = use_bias)\n",
    "        self.swiglulayer = SwiGLULayer(dim = 1)\n",
    "        self.drop = nn.Dropout(rate)\n",
    "        self.linear2 = nn.Linear(swiglu_out,embedding_dim, bias = use_bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layernorm(inputs.transpose(1,2)) # 차원바뀌고 채널 dim=2\n",
    "        x = self.linear1(x) \n",
    "        x = self.swiglulayer(x.transpose(1,2)) # 또 차원 바뀌고 채널 dim =1\n",
    "        x = self.drop(x)\n",
    "        x = self.linear2(x.transpose(1,2)) # 차원 바뀌고 채널 dim=2\n",
    "        out = self.drop(x.transpose(1,2)) # 차원 또 바뀌고 채널 dim =1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdHfQ6HHw4UP",
    "outputId": "966a8466-6c37-415f-cb16-2e13189dbb78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForwardSwiGLU(embedding_dim = 5, mult = 4)\n",
    "x = torch.randn(3,5,10)\n",
    "print(ffn(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rke3X-N6-n9S"
   },
   "source": [
    "### CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "X73TEopBGYov"
   },
   "outputs": [],
   "source": [
    "n_train = 6400689\n",
    "model = FeedForwardSwiGLU(embedding_dim = 5, mult = 4)\n",
    "optim           = torch.optim.Adam(model.parameters(), lr = ARGS['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "scheduler       = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=ARGS['max_lr'],pct_start = 0.1, \n",
    "                                                                   steps_per_epoch=int(n_train/ARGS['batch_size'])+1, epochs=ARGS['epochs'],anneal_strategy='cos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "r-w_RbKWIXeM",
    "outputId": "b1995ed8-b95f-4da8-9b01-707fb00c2772"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'OneCycleLR Scheduler')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs  = []\n",
    "for  i  in  range(250000):\n",
    "\tscheduler.step()\n",
    "\tlrs.append(optim.param_groups[0][\"lr\"])  \n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"OneCycleLR Scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGqeHeQRxxLY"
   },
   "source": [
    "### ConformerSASwiGLULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "saxlznhLxxLY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConformerSASwiGLULayer(nn.Module):\n",
    "    def __init__(self, embedding_dim,  ff_mult = 4, kernel_size = 15, rate = 0.2, num_heads = 4, use_bias = False):\n",
    "        super(ConformerSASwiGLULayer, self).__init__()\n",
    "        self.ff1 = FeedForwardSwiGLU(embedding_dim = embedding_dim, mult = ff_mult, rate = rate, use_bias = use_bias)\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim,eps = 1e-6)\n",
    "        self.conv = nn.Sequential(   \n",
    "          nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, groups=embedding_dim, padding='same'),\n",
    "          nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=1, padding='same'),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(rate),\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim,eps = 1e-6)    \n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.ff2 = FeedForwardSwiGLU(embedding_dim = embedding_dim, mult = ff_mult, rate = rate, use_bias = use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        x = self.layernorm1(x.transpose(1,2)) #채널 dim = 2\n",
    "        x = x + self.conv(x.transpose(1, 2)).transpose(1, 2) # output 채널 dim = 2\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.attn(x, x, x)[0]\n",
    "        x = x.transpose(1,2) + 0.5 * self.ff2(x.transpose(1,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5mMf8GPxxLZ",
    "outputId": "92e9877e-f3ac-49da-b3f5-325d74b60719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 10])\n"
     ]
    }
   ],
   "source": [
    "layer = ConformerSASwiGLULayer(embedding_dim = 16)\n",
    "x = torch.randn(3,16,10)\n",
    "print(layer(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKbk2vhOTgA5"
   },
   "source": [
    "### SequenceMaskLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q7WXIVfClKlE"
   },
   "outputs": [],
   "source": [
    "class SequenceMaskLayer(nn.Module):\n",
    "    def __init__(self, n_positions, ratio = 0.2):\n",
    "        super(SequenceMaskLayer, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.n_positions = n_positions\n",
    "        self.N = 4\n",
    "        self.M = 5\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.ratio > 0:\n",
    "            m = torch.rand(x.shape) < self.ratio\n",
    "            m = m*1\n",
    "            is_valid = x == self.N\n",
    "            is_valid = is_valid * 1\n",
    "            m = m * is_valid\n",
    "            x0 = torch.ones(x.shape) * self.M\n",
    "\n",
    "            x = m * x0 + (1 - m) * x\n",
    "            m = m.float()\n",
    "        else:\n",
    "            m = torch.zeros(x.shape)\n",
    "    \n",
    "        return x, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOj2tUQmTgA6",
    "outputId": "8a271106-304b-4b95-83a1-232664591280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0., 0., 0., 4., 4., 1., 0., 4., 1., 0., 2., 1., 1., 0., 4., 2., 0.,\n",
      "         0., 3.],\n",
      "        [0., 2., 2., 3., 3., 0., 1., 3., 1., 3., 4., 2., 1., 5., 5., 1., 1., 0.,\n",
      "         2., 3.],\n",
      "        [2., 1., 1., 1., 3., 1., 0., 1., 4., 3., 3., 2., 3., 4., 0., 4., 2., 1.,\n",
      "         0., 2.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "layer = SequenceMaskLayer(n_positions = 20, ratio = 0.2)\n",
    "x = torch.rand([3,20])*5\n",
    "x = torch.floor(x)\n",
    "x, m = layer(x)\n",
    "print(x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGFLv3peCCKS"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "om_YoxPICCKS"
   },
   "outputs": [],
   "source": [
    "input_dim = int(6) # A,C,G,T,N,M\n",
    "#input_dim = int(5) # A,C,G,T,N\n",
    "n_positions = ARGS['max_width'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbU-MCtK1B7k",
    "outputId": "3129bc87-a86a-49c5-b203-fa4af6e201ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampled dataset size: 6737568\n",
      "training dataset size: 6400689\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data['seq']))\n",
    "n_train = int(n * ARGS['train_split'])\n",
    "print('downsampled dataset size: %d' % (n))\n",
    "print('training dataset size: %d' % (n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQBqfMrpCCKT",
    "outputId": "51b10fec-522a-42f8-d692-4fa9e8eb19b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 6400689\n",
      "# val samples: 336879\n"
     ]
    }
   ],
   "source": [
    "train_data = {'seq':data['seq'][:n_train],'expression':data['expression'][:n_train]}\n",
    "val_data = {'seq':data['seq'][n_train:],'expression':data['expression'][n_train:]}\n",
    "\n",
    "print('# training samples: %d' % (len(train_data['seq'])))\n",
    "print('# val samples: %d' % (len(val_data['seq'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2xtZGxPzz4s"
   },
   "source": [
    "# DataLoader & TestSet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O5LDHmegztZL"
   },
   "outputs": [],
   "source": [
    "class train_loader(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data['seq']\n",
    "        self.data_label = data['expression']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data[index]), self.data_label[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class test_loader(object):\n",
    "    def __init__(self,args):\n",
    "        lines = open(\"/Data1/PGE/torch_ti/filtered_test_data_with_MAUDE_expression.txt\", \"r\").read().splitlines()\n",
    "        data = [x.split('\\t')[0] for x in lines]\n",
    "        data_label = [x.split('\\t')[1] for x in lines]\n",
    "        df = pd.DataFrame()\n",
    "        df['dna'] = data\n",
    "        df['expression'] = data_label\n",
    "        df['dna'] = df['dna'].astype('string')\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        print('number of unique sequences in the first {} positions: {}'.format(args['head_len'], len(df['dna'].str[:args['head_len']].unique())))\n",
    "        print('number of unique sequences in the last {} positions: {}'.format(args['tail_len'], len(df['dna'].str[-args['tail_len']:].unique())))\n",
    "        df['dna'] = df['dna'].str[args['head_len']:]\n",
    "        df['dna'] = df['dna'].str[:-args['tail_len']]\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        assert all(df['len'] <= args['max_width'])\n",
    "        \n",
    "        df['dna'] = df['dna'].str.pad(width = args['max_width'], side = 'both', fillchar = 'N')\n",
    "        df['dna'] = df['dna'] + df['dna'].apply(lambda x: str(Seq(x).reverse_complement())).astype('string')\n",
    "        \n",
    "        input_dim = int(6) # A,C,G,T,N,M\n",
    "        n_positions = int(args['max_width'] * 2)\n",
    "        self.dna = np.empty((0, n_positions), np.uint8)\n",
    "        for x in np.array_split(df['dna'], 10): # split data into chunks\n",
    "            y = np.array(x.apply(list))\n",
    "            y = np.vstack(y)\n",
    "            y = np.vectorize(ARGS['alphabets'].get)(y)\n",
    "            y = y.astype(np.uint8)\n",
    "            print(y.shape)\n",
    "            self.dna = np.append(self.dna, y, axis = 0)\n",
    "        print(self.dna.shape)\n",
    "        self.expression = df['expression'].astype('float32').to_numpy()\n",
    "        expression_std = np.std(self.expression)\n",
    "        expression_mean = np.mean(self.expression)\n",
    "        self.expression = (self.expression - expression_mean) / expression_std\n",
    "        \n",
    "        print(self.expression.shape)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.dna[index]), self.expression[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KwOLqNOwU3U"
   },
   "source": [
    "## The regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, strides = 2, ratio = 0.2, ff_mult = 4, use_bias = False, num_projectors = 8):\n",
    "        super(FirstLayer_Block, self).__init__()\n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.input_dim = input_dim\n",
    "        self.kmer = kmer\n",
    "        self.strides = strides\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        self.masking = SequenceMaskLayer(n_positions = n_positions, ratio = ratio)\n",
    "        self.pos_embedding = nn.Embedding(self.n_positions, embedding_dim)\n",
    "        self.strand_embedding = nn.Embedding(2, embedding_dim) # plus/minus strands\n",
    "        self.expression_embedding = nn.Linear(1,embedding_dim)\n",
    "        self.kmer_dense = nn.Linear(input_dim*self.kmer,embedding_dim)\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = F.one_hot(x.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "\n",
    "        x = x.unsqueeze(2)  # b,seq,em,1\n",
    "        x_shape = x.shape\n",
    "        fold_shape = x.unfold(1,self.kmer,self.strides).transpose(3,4).shape\n",
    "        div = x_shape[1] - fold_shape[1]\n",
    "        x = F.pad(x.unfold(1,self.kmer,self.strides).transpose(3,4),(0,0,0,0,0,0,0,div),'constant',0).reshape(x.shape[0],x.shape[1]//self.strides,x.shape[2],-1)\n",
    "        x = x.squeeze(2).float()\n",
    "        x = self.kmer_dense(x)\n",
    "\n",
    "        pos = torch.arange(start=0, end = self.n_positions, step=1).cuda()\n",
    "        pos = pos.unsqueeze(0)\n",
    "        pos = self.pos_embedding(pos.long())\n",
    "\n",
    "        strand = torch.tensor(np.repeat([0,1], repeats = int(self.n_positions / 2))).cuda()\n",
    "        strand = strand.unsqueeze(0)\n",
    "        strand = self.strand_embedding(strand.long())\n",
    "\n",
    "        x = x + pos + strand  # 채널 dim=2\n",
    "\n",
    "        expression = torch.zeros((batch_size, self.num_projectors, 1)).cuda()\n",
    "        expression = self.expression_embedding(expression.float())\n",
    "\n",
    "        x = torch.cat([expression, x], dim = 1)\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Core_Block(nn.Module):\n",
    "    def __init__(self, embedding_dim = 32, input_dim = 5, n_blocks = 4, \n",
    "               kernel_size =15, rate = 0.2, num_heads = 4):\n",
    "        super(Core_Block, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.blocks = nn.ModuleList([ConformerSASwiGLULayer(embedding_dim = embedding_dim,\n",
    "                                    kernel_size = kernel_size, rate = rate, num_heads = num_heads) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        for i in range(self.n_blocks) :\n",
    "            x = self.blocks[i](x)\n",
    "        \n",
    "        return x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, embedding_dim = 32, input_dim = 5, rate = 0.2, strides = 2, use_bias = False, num_projectors = 8):\n",
    "        super(FinalLayer_Block, self).__init__()\n",
    "        \n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.expression_dense = nn.Linear(embedding_dim,1)\n",
    "        self.nucleotide_dense = nn.Linear(embedding_dim,input_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        expression = x[:,:self.num_projectors,:]\n",
    "        x = x[:, -self.n_positions:, :]\n",
    "\n",
    "        expression = self.dropout(expression)\n",
    "        expression = self.expression_dense(expression)\n",
    "        expression = torch.mean(expression, 1)\n",
    "\n",
    "        x = self.nucleotide_dense(x)\n",
    "\n",
    "        return expression, x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, n_blocks = 4, \n",
    "               kernel_size =15, rate = 0.2, strides = 2, ratio = 0.2, num_heads = 4, ff_mult = 4, \n",
    "               use_bias = False, num_projectors = 8):\n",
    "        super(Regressor, self).__init__()\n",
    "        \n",
    "        self.masking = SequenceMaskLayer(n_positions = n_positions, ratio = ratio)\n",
    "        \n",
    "        self.first_block = FirstLayer_Block(n_positions, kmer, embedding_dim, input_dim, strides, \n",
    "                                            ratio, ff_mult, use_bias, num_projectors)\n",
    "        self.core_block = Core_Block(embedding_dim, input_dim, n_blocks, \n",
    "                                     kernel_size, rate, num_heads)\n",
    "        self.final_block = FinalLayer_Block(n_positions, embedding_dim, input_dim, rate,\n",
    "                                       strides, use_bias, num_projectors)\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq)\n",
    "\n",
    "        first_out = self.first_block(x)\n",
    "        core_out = self.core_block(first_out)\n",
    "        expression, seq_out = self.final_block(core_out)\n",
    "\n",
    "        return expression, seq_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "z-jAAiPA0oMS"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "\tdef __init__(self, args,**kwargs):\n",
    "\t\tsuper(RegressorModel, self).__init__()\n",
    "\t\t## regressor\n",
    "\t\tself.arg = args\n",
    "\t\tself.regressor = Regressor(n_positions = n_positions,embedding_dim = args['embedding_dim'],\n",
    "                             n_blocks = args['n_blocks_regressor'],kmer = args['kmer'],input_dim = input_dim,\n",
    "                             strides = args['strides'],ratio = args['mask_ratio'],num_heads = args['num_heads'],\n",
    "                             rate = args['dropout_rate'],num_projectors = args['num_projectors']).cuda()\n",
    "\t\tself.mse_loss = nn.MSELoss(reduction='none').cuda()\n",
    "\t\tself.scc_loss = nn.CrossEntropyLoss( reduction='none').cuda()\n",
    "\t\tself.optim           = torch.optim.Adam(self.regressor.parameters(), lr = args['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "\t\tself.scheduler       = torch.optim.lr_scheduler.OneCycleLR(self.optim, max_lr=args['max_lr'],pct_start = 0.05, \n",
    "                                                                   steps_per_epoch=int(n_train/args['batch_size'])+1, epochs=args['epochs'],anneal_strategy='cos')\n",
    "\t\tprint(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number(백만) = %.2f\"%(sum(param.numel() for param in self.regressor.parameters()) / 1024 / 1024))\n",
    "\n",
    "\tdef train_network(self, epoch, loader):\n",
    "\t\tself.train()\n",
    "\t\t## Update the learning rate based on the current epoch\n",
    "\t\tif epoch > 0 :\n",
    "\t\t\tself.scheduler.step((epoch - 1)*int(n_train/self.arg['batch_size']))\n",
    "\t\t\tprint('LR : ',self.scheduler.get_last_lr()[0])\n",
    "\t\tindex, loss = 0, 0\n",
    "\t\tfor num, (data, labels) in tqdm(enumerate(loader, start = 1)):\n",
    "\n",
    "\t\t\tself.zero_grad()\n",
    "\t\t\tseq, mask = self.regressor.masking(data)\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\t\texpression, seq_pred = self.regressor.forward(data.cuda()) \n",
    "\t\t\tloss_expression = self.mse_loss(labels.to(torch.float32), expression.squeeze(1).to(torch.float32))\n",
    "\t\t\tloss_seq = mask.cuda() * self.scc_loss(seq_pred,data.long().cuda())\n",
    "\t\t\tloss_seq = torch.sum(loss_seq) / (torch.sum(mask.cuda()) + 1)\n",
    "\t\t\tnloss = (loss_expression.to(torch.float32) + loss_seq.to(torch.float32)).mean().to(torch.float32)\n",
    "\t\t\t\n",
    "\t\t\tnloss.backward()\n",
    "\t\t\tself.optim.step()\n",
    "\t\t\tself.scheduler.step()\n",
    "\t\t\tlr = self.scheduler.get_last_lr()[0]\n",
    "\t\t\tindex += len(labels)\n",
    "\t\t\tloss += nloss.detach().cpu().numpy()\n",
    "\t\t\tif num % 200 == 0 :\n",
    "\t\t\t\tsys.stderr.write(time.strftime(\"%m-%d %H:%M:%S\") + \\\n",
    "\t\t\t\t\" [%2d] Lr: %5f, Training: %.2f%%, \"    %(epoch, lr, 100 * (num / loader.__len__())) + \\\n",
    "\t\t\t\t\" Loss: %.5f \\r\"        %(loss/(num)))\n",
    "\t\t\t\tsys.stderr.flush()\n",
    "\t\t\t\tsys.stdout.write(\"\\n\")\n",
    "\n",
    "\t\treturn loss/num, lr \n",
    "\n",
    "\tdef eval_network(self, loader):\n",
    "\t\tself.eval()\n",
    "\t\texp = []\n",
    "\t\treal_exp = []\n",
    "\t\tfor idx, (data,labels) in tqdm(enumerate(loader)):\n",
    "\t\t\tdata_1 = torch.FloatTensor(data).cuda()\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\texpression, _ = self.regressor.forward(data_1)\n",
    "\t\t\tif len(expression.shape) > 1 :\n",
    "\t\t\t\texpression = expression.reshape(-1)\n",
    "\t\t\t\texpression = expression.detach().cpu().numpy()\n",
    "\t\t\t\tlabels = labels.detach().cpu().numpy() \n",
    "\t\t\texp.append(expression)\n",
    "\t\t\treal_exp.append(labels)\n",
    "\t\t\t\n",
    "\t\t# Coumpute Metric\n",
    "\t\texp = np.array(exp).reshape(-1)\n",
    "\t\treal_exp = np.array(real_exp).reshape(-1)\n",
    "\t\tPR = pearson_r(exp, real_exp)\n",
    "\n",
    "\t\treturn PR\n",
    "\n",
    "\tdef save_parameters(self, path):\n",
    "\t\ttorch.save(self.state_dict(), path)\n",
    "\n",
    "\tdef load_parameters(self, path):\n",
    "\t\tself_state = self.state_dict()\n",
    "\t\tloaded_state = torch.load(path)\n",
    "\t\tfor name, param in loaded_state.items():\n",
    "\t\t\torigname = name\n",
    "\t\t\tif name not in self_state:\n",
    "\t\t\t\tname = name.replace(\"module.\", \"\")\n",
    "\t\t\t\tif name not in self_state:\n",
    "\t\t\t\t\tprint(\"%s is not in the model.\"%origname)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tif self_state[name].size() != loaded_state[origname].size():\n",
    "\t\t\t\tprint(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tself_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOtXgSxYvy0F"
   },
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGLEOzKhpPE2"
   },
   "source": [
    "### Epoch 1 \n",
    "\n",
    "initial epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VpPeGE-OeX-",
    "outputId": "0cf7da67-67f8-4b6d-b151-a0c481d9fcb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n",
      "Current cuda device: 1\n",
      "number of unique sequences in the first 17 positions: 1\n",
      "number of unique sequences in the last 13 positions: 1\n",
      "(7111, 200)\n",
      "(7111, 200)\n",
      "(7111, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7110, 200)\n",
      "(71103, 200)\n",
      "(71103,)\n",
      "03-10 12:21:11 Model para number(백만) = 17.19\n",
      "LR :  1.200000000000002e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [06:15,  1.90s/it]03-10 12:27:26 [ 1] Lr: 0.000012, Training: 1.60%,  Loss: 1.04838 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "232it [07:18,  1.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m score_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(score_save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     50\u001b[0m \t\u001b[38;5;66;03m## Training for one epoch\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \tloss, lr \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \t\u001b[38;5;66;03m## Evaluation every [test_step] epochs\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mRegressorModel.train_network\u001b[0;34m(self, epoch, loader)\u001b[0m\n\u001b[1;32m     47\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m---> 49\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[1;32m     51\u001b[0m \tsys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     52\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m%2d\u001b[39;00m\u001b[38;5;124m] Lr: \u001b[39m\u001b[38;5;132;01m%5f\u001b[39;00m\u001b[38;5;124m, Training: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;241m%\u001b[39m(epoch, lr, \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (num \u001b[38;5;241m/\u001b[39m loader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m())) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     53\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m        \u001b[38;5;241m%\u001b[39m(loss\u001b[38;5;241m/\u001b[39m(num)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the main code of the ECAPATDNN project, to define the parameters and build the construction\n",
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import argparse, glob, os, torch, warnings, time\n",
    "\n",
    "\n",
    "model_save_path = \"exps/model_reproducing_3module\"\n",
    "score_save_path = \"exps/score_reproducing_3module.txt\"\n",
    "os.makedirs(model_save_path,exist_ok = True)\n",
    "\n",
    "device = ARGS['device']\n",
    "torch.cuda.set_device(device)\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "\n",
    "\n",
    "## Define the data loader\n",
    "trainloader = train_loader(train_data)\n",
    "trainLoader = torch.utils.data.DataLoader(trainloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "valloader = train_loader(val_data)\n",
    "valLoader = torch.utils.data.DataLoader(valloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "testloader = test_loader(ARGS)\n",
    "testLoader = torch.utils.data.DataLoader(testloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "\n",
    "## Search for the exist models\n",
    "modelfiles = glob.glob('%s/model_0*.model'%model_save_path)\n",
    "modelfiles.sort()\n",
    "\n",
    "## Otherwise, system will try to start from the saved model&epoch\n",
    "if len(modelfiles) >= 1:\n",
    "\tprint(\"Model %s loaded from previous state!\"%modelfiles[-1])\n",
    "\tepoch = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][6:]) + 1\n",
    "\ts = RegressorModel(ARGS)\n",
    "\ts.load_parameters(modelfiles[-1])\n",
    "\teval_pr_ = s.eval_network(testLoader)\n",
    "\tprint('Previous Eval Pearson_R : ',eval_pr_)\n",
    "## Otherwise, system will train from scratch\n",
    "else:\n",
    "\tepoch = 1\n",
    "\ts = RegressorModel(ARGS)\n",
    "\n",
    "pr = []\n",
    "eval_pr = []\n",
    "score_file = open(score_save_path, \"a+\")\n",
    "\n",
    "while(1):\n",
    "\t## Training for one epoch\n",
    "\tloss, lr = s.train_network(epoch = epoch, loader = trainLoader)\n",
    "\n",
    "\t## Evaluation every [test_step] epochs\n",
    "\tif epoch % 1 == 0:\n",
    "\t\ts.save_parameters(model_save_path + \"/model_%04d.model\"%epoch)\n",
    "\t\tpr.append(s.eval_network(valLoader))\n",
    "\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, Pearson_R %2.2f%%, bestPearson_R %2.2f%%\"%(epoch, pr[-1], max(pr)))\n",
    "\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, Pearson_R %2.2f%%, bestPearson_R %2.2f%%\\n\"%(epoch, lr, loss, pr[-1], max(pr)))\n",
    "\t\tscore_file.flush()\n",
    "\t\tif pr[-1] == max(pr) :\n",
    "\t\t\ts.save_parameters(model_save_path + \"/model_best.model\")\n",
    "\t\t\teval_pr.append(s.eval_network(testLoader))\n",
    "\t\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, Eval_Pearson_R %2.2f%%, Best_Eval_Pearson_R %2.2f%%\"%(epoch, eval_pr[-1], max(eval_pr)))\n",
    "\t\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, Pearson_R %2.2f%%, Best_Eval_Pearson_R %2.2f%%\\n\"%(epoch, lr, loss, eval_pr[-1], max(eval_pr)))\n",
    "\t\t\tscore_file.flush()\n",
    "\n",
    "\tif epoch >= ARGS['epochs']:\n",
    "\t\tquit()\n",
    "\n",
    "\tepoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp22H950YtNt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19E3HT1ESJ4A"
   },
   "source": [
    "## Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "53wVoagb4tD8",
    "outputId": "74fa0970-dc30-492f-9cc9-76ce90418ec9"
   },
   "outputs": [],
   "source": [
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFWCf4x-5bTE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
