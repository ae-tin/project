{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jDe-m_5aq4"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH9oF2RleEYl"
   },
   "source": [
    "## Load Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XamZpd5EeGHR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import session_info\n",
    "import pdb\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w9lA5-iVI0d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import pickle as pk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl2lXOAv9ljx"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "D0pjVLfcfMl7"
   },
   "outputs": [],
   "source": [
    "RUNTIME = 'none'\n",
    "ARGS = {\n",
    "  'model_id' : 'm20220727e',\n",
    "  'global_seed' : 123,\n",
    "  'shuffle_size' : 1000,\n",
    "  'max_width' : 100,\n",
    "  'head_len' : 17,\n",
    "  'tail_len' : 13,\n",
    "  'pct_ds' : 1, # % of total data for training/testing,\n",
    "  'train_split' : 0.95,\n",
    "  'alphabets' : {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3, 'N' : 4, 'M' : 5},\n",
    "  'initial_lr' : 1e-15,\n",
    "  'max_lr' : 23e-5,\n",
    "  'initial_epoch': 0,\n",
    "  'epochs' : 20,\n",
    "  'batch_size' : 512,\n",
    "  'dropout_rate' : 0.1,\n",
    "  'kmer': 10,\n",
    "  'strides' : 1,\n",
    "  'embedding_dim' : 512,\n",
    "  'num_heads' : 8,\n",
    "  'ff_mult' : 4,\n",
    "  'num_projectors' : 32,\n",
    "  'n_blocks_regressor' : 4,\n",
    "  'warmup_steps' : 12500, # ~ 1 epoch\n",
    "  'mask_ratio' : 0.05,\n",
    "  'remote_sample_submission_file' : 'https://raw.githubusercontent.com/de-Boer-Lab/DREAM-2022/main/sample_submission.json',\n",
    "  'eval' : True,\n",
    "  'device':'cuda:4',\n",
    "  'local_data_dir' : '/Data1/PGE/torch_ti/data/'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ZhZvwWifIpzY"
   },
   "outputs": [],
   "source": [
    "with open(ARGS['local_data_dir']+\"data.pk\",\"rb\") as fr:\n",
    "    data = pk.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i5XrE1H2Iwq"
   },
   "source": [
    "### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5LTFxNSIWQID"
   },
   "outputs": [],
   "source": [
    "np.random.seed(ARGS['global_seed'])\n",
    "torch.manual_seed(ARGS['global_seed'])\n",
    "python_random.seed(ARGS['global_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZLHuW06Tcm"
   },
   "source": [
    "### pearson_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "bDou1j1A6S2j"
   },
   "outputs": [],
   "source": [
    "def pearson_r(x, y):\n",
    "    x = torch.tensor(x,dtype=torch.float32)\n",
    "    y = torch.tensor(y,dtype=torch.float32)\n",
    "    mx = torch.mean(x, axis = 0, keepdims = True)\n",
    "    my = torch.mean(y, axis = 0, keepdims = True)\n",
    "    xm = x - mx\n",
    "    ym = y - my\n",
    "    t1_norm = F.normalize(xm, p=2, dim=0)\n",
    "    t2_norm = F.normalize(ym, p=2, dim=0)\n",
    "    return torch.sum(torch.mul(t1_norm, t2_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E4kvC_v6oYG",
    "outputId": "53ce49be-7138-490b-ba8f-85a92dc3592e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson r (stats.pearsonr): -0.09270195576139686\n",
      "pearson r (pearson_r): -0.09270194172859192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2457/2976460682.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x,dtype=torch.float32)\n",
      "/tmp/ipykernel_2457/2976460682.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "print('pearson r (stats.pearsonr): {}'.format(stats.pearsonr(x, y)[0]))\n",
    "print('pearson r (pearson_r): {}'.format(pearson_r(torch.unsqueeze(torch.Tensor(x),1), torch.unsqueeze(torch.Tensor(y),1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGFLv3peCCKS"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbU-MCtK1B7k",
    "outputId": "3129bc87-a86a-49c5-b203-fa4af6e201ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampled dataset size: 6737568\n",
      "training dataset size: 6400689\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data['seq']))\n",
    "n_train = int(n * ARGS['train_split'])\n",
    "print('downsampled dataset size: %d' % (n))\n",
    "print('training dataset size: %d' % (n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQBqfMrpCCKT",
    "outputId": "51b10fec-522a-42f8-d692-4fa9e8eb19b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 6400689\n",
      "# val samples: 336879\n"
     ]
    }
   ],
   "source": [
    "train_data = {'seq':data['seq'][:n_train],'expression':data['expression'][:n_train]}\n",
    "val_data = {'seq':data['seq'][n_train:],'expression':data['expression'][n_train:]}\n",
    "\n",
    "print('# training samples: %d' % (len(train_data['seq'])))\n",
    "print('# val samples: %d' % (len(val_data['seq'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2xtZGxPzz4s"
   },
   "source": [
    "# DataLoader & TestSet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class train_loader(object):\n",
    "    def __init__(self, data):\n",
    "        self.kmer = 10\n",
    "        self.strides = 1\n",
    "        self.input_dim = 6\n",
    "        self.seq = data['seq']\n",
    "        self.data = torch.tensor(data['seq'])\n",
    "        self.data_label = data['expression']\n",
    "\n",
    "        x = F.one_hot(self.data.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "        self.data2 = x.transpose(1,2).numpy()\n",
    "\n",
    "#        print('trn_data_shape : ',self.data2.shape)\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data2[index]), self.data_label[index], torch.FloatTensor(self.seq[index]) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class test_loader(object):\n",
    "    def __init__(self,args):\n",
    "        lines = open(\"/Data1/PGE/torch_ti/filtered_test_data_with_MAUDE_expression.txt\", \"r\").read().splitlines()\n",
    "        data = [x.split('\\t')[0] for x in lines]\n",
    "        data_label = [x.split('\\t')[1] for x in lines]\n",
    "        df = pd.DataFrame()\n",
    "        df['dna'] = data\n",
    "        df['expression'] = data_label\n",
    "        df['dna'] = df['dna'].astype('string')\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        print('number of unique sequences in the first {} positions: {}'.format(args['head_len'], len(df['dna'].str[:args['head_len']].unique())))\n",
    "        print('number of unique sequences in the last {} positions: {}'.format(args['tail_len'], len(df['dna'].str[-args['tail_len']:].unique())))\n",
    "        df['dna'] = df['dna'].str[args['head_len']:]\n",
    "        df['dna'] = df['dna'].str[:-args['tail_len']]\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        assert all(df['len'] <= args['max_width'])\n",
    "        \n",
    "        df['dna'] = df['dna'].str.pad(width = args['max_width'], side = 'both', fillchar = 'N')\n",
    "        df['dna'] = df['dna'] + df['dna'].apply(lambda x: str(Seq(x).reverse_complement())).astype('string')\n",
    "        \n",
    "        input_dim = int(6) # A,C,G,T,N,M\n",
    "        n_positions = int(args['max_width'] * 2)\n",
    "        self.dna = np.empty((0, n_positions), np.uint8)\n",
    "        for x in np.array_split(df['dna'], 10): # split data into chunks\n",
    "            y = np.array(x.apply(list))\n",
    "            y = np.vstack(y)\n",
    "            y = np.vectorize(ARGS['alphabets'].get)(y)\n",
    "            y = y.astype(np.uint8)\n",
    "            print(y.shape)\n",
    "            self.dna = np.append(self.dna, y, axis = 0)\n",
    "        print(self.dna.shape)\n",
    "        self.expression = df['expression'].astype('float32').to_numpy()\n",
    "        expression_std = np.std(self.expression)\n",
    "        expression_mean = np.mean(self.expression)\n",
    "        self.expression = (self.expression - expression_mean) / expression_std\n",
    "        \n",
    "        print(self.expression.shape)\n",
    "        \n",
    "        \n",
    "        self.kmer = 10\n",
    "        self.strides = 1\n",
    "        self.input_dim = 6\n",
    "        self.data = torch.tensor(self.dna)\n",
    "        \n",
    "        x = F.one_hot(self.data.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "        self.data2 = x.transpose(1,2).numpy()  # output = (b,embed,seq)\n",
    "        \n",
    "#        print('test_data_shape : ',self.data2.shape)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data2[index]), self.expression[index], torch.FloatTensor(self.dna[index]) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "z-jAAiPA0oMS"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from prixfixe.unlockdna import (UnlockDNA_CoreBlock,\n",
    "                      UnlockDNA_FirstLayersBlock,\n",
    "                      UnlockDNA_FinalLayersBlock)\n",
    "\n",
    "from prixfixe.prixfixe import PrixFixeNet\n",
    "\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "\tdef __init__(self, args,**kwargs):\n",
    "\t\tsuper(RegressorModel, self).__init__()\n",
    "\t\t## regressor\n",
    "\t\tself.arg = args\n",
    "\t\tfirst = UnlockDNA_FirstLayersBlock(in_channels=6,\n",
    "                                   out_channels=512, \n",
    "                                   seqsize=200)\n",
    "\t\tcore = UnlockDNA_CoreBlock(in_channels=512,\n",
    "                         out_channels =512,\n",
    "                         seqsize=232)        \n",
    "\t\tfinal = UnlockDNA_FinalLayersBlock(in_channels=512, \n",
    "                                 seqsize=232)       \n",
    "\t\t        \n",
    "\t\tself.regressor = PrixFixeNet(first=first,core=core,final=final).cuda()\n",
    "\t\tself.mse_loss = nn.MSELoss(reduction='none').cuda()\n",
    "\t\tself.scc_loss = nn.CrossEntropyLoss( reduction='none').cuda()\n",
    "\t\tself.optim           = torch.optim.Adam(self.regressor.parameters(), lr = args['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "\t\tself.scheduler       = torch.optim.lr_scheduler.OneCycleLR(self.optim, max_lr=ARGS['max_lr'],pct_start = 0.04, \n",
    "                                                                   steps_per_epoch=ARGS['warmup_steps'], epochs=ARGS['epochs']+5,anneal_strategy='cos')\n",
    "\t\tprint(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number(백만) = %.2f\"%(sum(param.numel() for param in self.regressor.parameters()) / 1024 / 1024))\n",
    "\n",
    "\tdef train_network(self, epoch, loader):\n",
    "\t\tself.train()\n",
    "\t\t## Update the learning rate based on the current epoch\n",
    "\t\tif epoch > 0 :\n",
    "\t\t\tself.scheduler.step((epoch - 1)*int(n_train/self.arg['batch_size']))\n",
    "\t\t\tprint('LR : ',self.scheduler.get_last_lr()[0])\n",
    "\t\tindex, loss = 0, 0\n",
    "\t\tfor num, (data, labels, sequence) in tqdm(enumerate(loader, start = 1)):\n",
    "\n",
    "\t\t\tself.zero_grad()\n",
    "\t\t\tseq, mask = self.regressor.masking(sequence)\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\t\texpression, seq_pred = self.regressor.forward(data.cuda()) \n",
    "\t\t\tloss_expression = self.mse_loss(labels.to(torch.float32), expression.squeeze(1).to(torch.float32))\n",
    "\t\t\tloss_seq = mask.cuda() * self.scc_loss(seq_pred,sequence.long().cuda()) #.long()\n",
    "\t\t\tloss_seq = torch.sum(loss_seq) / (torch.sum(mask.cuda()) + 1)\n",
    "\t\t\tnloss = (loss_expression.to(torch.float32) + loss_seq.to(torch.float32)).mean().to(torch.float32)\n",
    "\t\t\t\n",
    "\t\t\tnloss.backward()\n",
    "\t\t\tself.optim.step()\n",
    "\t\t\tself.scheduler.step()\n",
    "\t\t\tlr = self.scheduler.get_last_lr()[0]\n",
    "\t\t\tindex += len(labels)\n",
    "\t\t\tloss += nloss.detach().cpu().numpy()\n",
    "\t\t\tif num % 500 == 0 :\n",
    "\t\t\t\tsys.stderr.write(time.strftime(\"%m-%d %H:%M:%S\") + \\\n",
    "\t\t\t\t\" [%2d] Lr: %5f, Training: %.2f%%, \"    %(epoch, lr, 100 * (num / loader.__len__())) + \\\n",
    "\t\t\t\t\" Loss: %.5f \\r\"        %(loss/(num)))\n",
    "\t\t\t\tsys.stderr.flush()\n",
    "\t\t\t\tsys.stdout.write(\"\\n\")\n",
    "\n",
    "\t\treturn loss/num, lr \n",
    "\n",
    "\tdef eval_network(self, loader):\n",
    "\t\tself.eval()\n",
    "\t\texp = []\n",
    "\t\treal_exp = []\n",
    "\t\tfor idx, (data,labels,_) in tqdm(enumerate(loader)):\n",
    "\t\t\tdata_1 = torch.FloatTensor(data).cuda()\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\texpression, _ = self.regressor.forward(data_1)\n",
    "\t\t\tif len(expression.shape) > 1 :\n",
    "\t\t\t\texpression = expression.reshape(-1)\n",
    "\t\t\t\texpression = expression.detach().cpu().numpy()\n",
    "\t\t\t\tlabels = labels.detach().cpu().numpy() \n",
    "\t\t\texp.append(expression)\n",
    "\t\t\treal_exp.append(labels)\n",
    "\t\t\t\n",
    "\t\t# Coumpute Metric\n",
    "\t\texp = np.array(exp).reshape(-1)\n",
    "\t\treal_exp = np.array(real_exp).reshape(-1)\n",
    "\t\tPR = pearson_r(exp, real_exp)\n",
    "\n",
    "\t\treturn PR\n",
    "\n",
    "\tdef save_parameters(self, path):\n",
    "\t\ttorch.save(self.state_dict(), path)\n",
    "\n",
    "\tdef load_parameters(self, path):\n",
    "\t\tself_state = self.state_dict()\n",
    "\t\tloaded_state = torch.load(path)\n",
    "\t\tfor name, param in loaded_state.items():\n",
    "\t\t\torigname = name\n",
    "\t\t\tif name not in self_state:\n",
    "\t\t\t\tname = name.replace(\"module.\", \"\")\n",
    "\t\t\t\tif name not in self_state:\n",
    "\t\t\t\t\tprint(\"%s is not in the model.\"%origname)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tif self_state[name].size() != loaded_state[origname].size():\n",
    "\t\t\t\tprint(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tself_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOtXgSxYvy0F"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VpPeGE-OeX-",
    "outputId": "0cf7da67-67f8-4b6d-b151-a0c481d9fcb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:4\n",
      "Current cuda device: 4\n",
      "number of unique sequences in the first 17 positions: 1\n",
      "number of unique sequences in the last 13 positions: 1\n",
      "(7111, 200)\n",
      "(7111, 200)\n",
      "(7111, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(71103, 200)\n",
      "(71103,)\n",
      "03-30 12:41:48 Model para number(백만) = 17.19\n",
      "Model exps/model_reproducing_3module/model_best.model loaded from previous state!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "138it [01:53,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Pearson R : 0.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import argparse, glob, os, torch, warnings, time\n",
    "\n",
    "\n",
    "model_save_path = \"exps/model_reproducing_3module\"\n",
    "score_save_path = \"exps/score_reproducing_3module.txt\"\n",
    "os.makedirs(model_save_path,exist_ok = True)\n",
    "\n",
    "device = ARGS['device']\n",
    "torch.cuda.set_device(device)\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "\n",
    "testloader = test_loader(ARGS)\n",
    "testLoader = torch.utils.data.DataLoader(testloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "\n",
    "## Search for the exist models\n",
    "modelfiles = glob.glob('%s/*.model'%model_save_path)\n",
    "modelfiles.sort()\n",
    "for i in modelfiles : \n",
    "    if 'best' in i :\n",
    "        evalfile_path = i\n",
    "\n",
    "## Only do evaluation, the initial_model is necessary\n",
    "if ARGS['eval'] == True :\n",
    "    s = RegressorModel(ARGS)\n",
    "    print(\"Model %s loaded from previous state!\"%evalfile_path)\n",
    "    s.load_parameters(evalfile_path)\n",
    "    PR = s.eval_network(testLoader)\n",
    "    print(\"Evaluation Pearson R : %2.2f%%\"%(PR))\n",
    "    quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp22H950YtNt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
