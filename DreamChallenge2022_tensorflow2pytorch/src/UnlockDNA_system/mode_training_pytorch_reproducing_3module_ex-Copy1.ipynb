{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jDe-m_5aq4"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH9oF2RleEYl"
   },
   "source": [
    "## Load Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XamZpd5EeGHR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import session_info\n",
    "import pdb\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9lA5-iVI0d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import pickle as pk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl2lXOAv9ljx"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0pjVLfcfMl7"
   },
   "outputs": [],
   "source": [
    "RUNTIME = 'none'\n",
    "ARGS = {\n",
    "  'model_id' : 'm20220727e',\n",
    "  'global_seed' : 123,\n",
    "  'shuffle_size' : 1000,\n",
    "  'max_width' : 100,\n",
    "  'head_len' : 17,\n",
    "  'tail_len' : 13,\n",
    "  'pct_ds' : 1, # % of total data for training/testing,\n",
    "  'train_split' : 0.95,\n",
    "  'alphabets' : {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3, 'N' : 4, 'M' : 5},\n",
    "  'initial_lr' : 1e-15,\n",
    "  'max_lr' : 23e-5,\n",
    "  'initial_epoch': 0,\n",
    "  'epochs' : 20,\n",
    "  'batch_size' : 512,\n",
    "  'dropout_rate' : 0.1,\n",
    "  'kmer': 10,\n",
    "  'strides' : 1,\n",
    "  'embedding_dim' : 512,\n",
    "  'num_heads' : 8,\n",
    "  'ff_mult' : 4,\n",
    "  'num_projectors' : 32,\n",
    "  'n_blocks_regressor' : 4,\n",
    "  'warmup_steps' : 12500, # ~ 1 epoch\n",
    "  'mask_ratio' : 0.05,\n",
    "  'remote_sample_submission_file' : 'https://raw.githubusercontent.com/de-Boer-Lab/DREAM-2022/main/sample_submission.json',\n",
    "  'eval' : False,\n",
    "  'device':'cuda:1'\n",
    "}\n",
    "if RUNTIME == 'msi':\n",
    "  ARGS['remote_data_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/training_data/pct_ds=' + str(ARGS['pct_ds']) + '/'\n",
    "  ARGS['local_data_dir'] = re.sub('https://', './', ARGS['remote_data_dir'])\n",
    "  ARGS['remote_checkpoint_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/notebooks_msi/' + ARGS['model_id'] + '/tf_ckpts/'\n",
    "  ARGS['remote_log_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/notebooks_msi/' + ARGS['model_id'] + '/log/'\n",
    "  ARGS['local_checkpoint_dir'] = re.sub('https://', './', ARGS['remote_checkpoint_dir'])\n",
    "  ARGS['local_log_dir'] = re.sub('https://', './', ARGS['remote_log_dir'])\n",
    "  ARGS['remote_test_data'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/test_sequences.txt.gz'\n",
    "  ARGS['local_test_data'] = re.sub('https://', './', ARGS['remote_test_data'])\n",
    "  ARGS['local_sample_submission_file'] = re.sub('https://', './', ARGS['remote_sample_submission_file'])\n",
    "  ARGS['remote_prediction_file'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/predictions/' + ARGS['model_id'] + '/pred.json'\n",
    "  ARGS['local_prediction_file'] = re.sub('https://', './', ARGS['remote_prediction_file'])\n",
    "  ARGS['s3_prediction_file'] = re.sub('https://s3.msi.umn.edu', 's3://', ARGS['remote_prediction_file'])\n",
    "  ARGS['remote_prediction_tsv_file'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/predictions/' + ARGS['model_id'] + '/pred.tsv'\n",
    "  ARGS['local_prediction_tsv_file'] = re.sub('https://', './', ARGS['remote_prediction_tsv_file'])\n",
    "  ARGS['s3_prediction_tsv_file'] = re.sub('https://s3.msi.umn.edu', 's3://', ARGS['remote_prediction_tsv_file'])\n",
    "else:\n",
    "  ARGS['local_data_dir'] = '/content/drive/MyDrive/training_data/pct_ds=' + str(ARGS['pct_ds']) + '/'\n",
    "  ARGS['local_checkpoint_dir'] = '/content/drive/MyDrive/' + ARGS['model_id'] + '/tf_ckpts/'\n",
    "  ARGS['local_log_dir'] = '/content/drive/MyDrive/' + ARGS['model_id'] + '/log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS['local_data_dir'] = '/Data1/PGE/torch_ti/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZhZvwWifIpzY"
   },
   "outputs": [],
   "source": [
    "with open(ARGS['local_data_dir']+\"data.pk\",\"rb\") as fr:\n",
    "    data = pk.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i5XrE1H2Iwq"
   },
   "source": [
    "### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5LTFxNSIWQID"
   },
   "outputs": [],
   "source": [
    "np.random.seed(ARGS['global_seed'])\n",
    "torch.manual_seed(ARGS['global_seed'])\n",
    "python_random.seed(ARGS['global_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZLHuW06Tcm"
   },
   "source": [
    "### pearson_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bDou1j1A6S2j"
   },
   "outputs": [],
   "source": [
    "def pearson_r(x, y):\n",
    "    x = torch.tensor(x,dtype=torch.float32)\n",
    "    y = torch.tensor(y,dtype=torch.float32)\n",
    "    mx = torch.mean(x, axis = 0, keepdims = True)\n",
    "    my = torch.mean(y, axis = 0, keepdims = True)\n",
    "    xm = x - mx\n",
    "    ym = y - my\n",
    "    t1_norm = F.normalize(xm, p=2, dim=0)\n",
    "    t2_norm = F.normalize(ym, p=2, dim=0)\n",
    "    return torch.sum(torch.mul(t1_norm, t2_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E4kvC_v6oYG",
    "outputId": "53ce49be-7138-490b-ba8f-85a92dc3592e"
   },
   "outputs": [],
   "source": [
    "x = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "print('pearson r (stats.pearsonr): {}'.format(stats.pearsonr(x, y)[0]))\n",
    "print('pearson r (pearson_r): {}'.format(pearson_r(torch.unsqueeze(torch.Tensor(x),1), torch.unsqueeze(torch.Tensor(y),1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrQ9T4-oSk8X"
   },
   "source": [
    "### GLULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oMX5LC1aSkRK"
   },
   "outputs": [],
   "source": [
    "class GLULayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(GLULayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out,gate = torch.chunk(x, 2, dim = self.dim)\n",
    "        return out * self.sig(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivpHjDlQSxUU",
    "outputId": "6f85a037-81fc-4924-cbb0-074aa9e0d808"
   },
   "outputs": [],
   "source": [
    "layer = GLULayer(dim = 1)\n",
    "x = torch.randn(3,6,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHdNKhzVvsMf"
   },
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeSyNETvvuex"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SwiGLULayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SwiGLULayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.swish = nn.SiLU() # same as swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, gate = torch.chunk(x, 2, dim = self.dim)\n",
    "        return out * self.swish(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuwI-YXjvuex",
    "outputId": "7e3be8c3-6e7a-4886-d85d-26c9c763b953"
   },
   "outputs": [],
   "source": [
    "layer = SwiGLULayer(dim = 1)\n",
    "x = torch.randn(3,6,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7BVgkw-w4UO"
   },
   "source": [
    "### FeedForwardSwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVdwwIxbw4UP"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardSwiGLU(nn.Module):\n",
    "    def __init__(self, embedding_dim, mult=4, rate = 0.0, use_bias = False):\n",
    "        super(FeedForwardSwiGLU, self).__init__()\n",
    "        swiglu_out = int(embedding_dim * mult/2)\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim,eps = 1e-6)\n",
    "        self.linear1 = nn.Linear(embedding_dim,embedding_dim * mult, bias = use_bias)\n",
    "        self.swiglulayer = SwiGLULayer(dim = 1)\n",
    "        self.drop = nn.Dropout(rate)\n",
    "        self.linear2 = nn.Linear(swiglu_out,embedding_dim, bias = use_bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layernorm(inputs.transpose(1,2)) # 차원바뀌고 채널 dim=2\n",
    "        x = self.linear1(x) \n",
    "        x = self.swiglulayer(x.transpose(1,2)) # 또 차원 바뀌고 채널 dim =1\n",
    "        x = self.drop(x)\n",
    "        x = self.linear2(x.transpose(1,2)) # 차원 바뀌고 채널 dim=2\n",
    "        out = self.drop(x.transpose(1,2)) # 차원 또 바뀌고 채널 dim =1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdHfQ6HHw4UP",
    "outputId": "966a8466-6c37-415f-cb16-2e13189dbb78"
   },
   "outputs": [],
   "source": [
    "ffn = FeedForwardSwiGLU(embedding_dim = 5, mult = 4)\n",
    "x = torch.randn(3,5,10)\n",
    "print(ffn(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rke3X-N6-n9S"
   },
   "source": [
    "### CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X73TEopBGYov"
   },
   "outputs": [],
   "source": [
    "n_train = 6400689\n",
    "model = FeedForwardSwiGLU(embedding_dim = 5, mult = 4)\n",
    "optim           = torch.optim.Adam(model.parameters(), lr = ARGS['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "scheduler       = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=ARGS['max_lr'],pct_start = 0.05, \n",
    "                                                                   steps_per_epoch=ARGS['warmup_steps'], epochs=ARGS['epochs']+2,anneal_strategy='cos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "r-w_RbKWIXeM",
    "outputId": "b1995ed8-b95f-4da8-9b01-707fb00c2772"
   },
   "outputs": [],
   "source": [
    "lrs  = []\n",
    "for  i  in  range(ARGS['warmup_steps']*(ARGS['epochs']+2)):\n",
    "\tscheduler.step()\n",
    "\tlrs.append(optim.param_groups[0][\"lr\"])  \n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"OneCycleLR Scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGqeHeQRxxLY"
   },
   "source": [
    "### ConformerSASwiGLULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "saxlznhLxxLY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConformerSASwiGLULayer(nn.Module):\n",
    "    def __init__(self, embedding_dim,  ff_mult = 4, kernel_size = 15, rate = 0.2, num_heads = 4, use_bias = False):\n",
    "        super(ConformerSASwiGLULayer, self).__init__()\n",
    "        self.ff1 = FeedForwardSwiGLU(embedding_dim = embedding_dim, mult = ff_mult, rate = rate, use_bias = use_bias)\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim,eps = 1e-6)\n",
    "        self.conv = nn.Sequential(   \n",
    "          nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, groups=embedding_dim, padding='same'),\n",
    "          nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=1, padding='same'),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(rate),\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim,eps = 1e-6)    \n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.ff2 = FeedForwardSwiGLU(embedding_dim = embedding_dim, mult = ff_mult, rate = rate, use_bias = use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        x = self.layernorm1(x.transpose(1,2)) #채널 dim = 2\n",
    "        x = x + self.conv(x.transpose(1, 2)).transpose(1, 2) # output 채널 dim = 2\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.attn(x, x, x)[0]\n",
    "        x = x.transpose(1,2) + 0.5 * self.ff2(x.transpose(1,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5mMf8GPxxLZ",
    "outputId": "92e9877e-f3ac-49da-b3f5-325d74b60719"
   },
   "outputs": [],
   "source": [
    "layer = ConformerSASwiGLULayer(embedding_dim = 16)\n",
    "x = torch.randn(3,16,10)\n",
    "print(layer(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKbk2vhOTgA5"
   },
   "source": [
    "### SequenceMaskLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7WXIVfClKlE"
   },
   "outputs": [],
   "source": [
    "class SequenceMaskLayer(nn.Module):\n",
    "    def __init__(self, n_positions, ratio = 0.2):\n",
    "        super(SequenceMaskLayer, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.n_positions = n_positions\n",
    "        self.N = 4\n",
    "        self.M = 5\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.ratio > 0:\n",
    "            m = torch.rand(x.shape) < self.ratio\n",
    "            m = m*1\n",
    "            is_valid = x == self.N\n",
    "            is_valid = is_valid * 1\n",
    "            m = m * is_valid\n",
    "            x0 = torch.ones(x.shape) * self.M\n",
    "\n",
    "            x = m * x0 + (1 - m) * x\n",
    "            m = m.float()\n",
    "        else:\n",
    "            m = torch.zeros(x.shape)\n",
    "    \n",
    "        return x, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOj2tUQmTgA6",
    "outputId": "8a271106-304b-4b95-83a1-232664591280"
   },
   "outputs": [],
   "source": [
    "layer = SequenceMaskLayer(n_positions = 20, ratio = 0.2)\n",
    "x = torch.rand([3,20])*5\n",
    "x = torch.floor(x)\n",
    "x, m = layer(x)\n",
    "print(x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGFLv3peCCKS"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "om_YoxPICCKS"
   },
   "outputs": [],
   "source": [
    "#input_dim = int(6) # A,C,G,T,N,M\n",
    "#input_dim = int(5) # A,C,G,T,N\n",
    "#n_positions = ARGS['max_width'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbU-MCtK1B7k",
    "outputId": "3129bc87-a86a-49c5-b203-fa4af6e201ce"
   },
   "outputs": [],
   "source": [
    "n = int(len(data['seq']))\n",
    "n_train = int(n * ARGS['train_split'])\n",
    "print('downsampled dataset size: %d' % (n))\n",
    "print('training dataset size: %d' % (n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQBqfMrpCCKT",
    "outputId": "51b10fec-522a-42f8-d692-4fa9e8eb19b5"
   },
   "outputs": [],
   "source": [
    "train_data = {'seq':data['seq'][:n_train],'expression':data['expression'][:n_train]}\n",
    "val_data = {'seq':data['seq'][n_train:],'expression':data['expression'][n_train:]}\n",
    "\n",
    "print('# training samples: %d' % (len(train_data['seq'])))\n",
    "print('# val samples: %d' % (len(val_data['seq'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2xtZGxPzz4s"
   },
   "source": [
    "# DataLoader & TestSet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class train_loader(object):\n",
    "    def __init__(self, data):\n",
    "        self.kmer = 10\n",
    "        self.strides = 1\n",
    "        self.input_dim = 6\n",
    "        self.seq = data['seq']\n",
    "        self.data = torch.tensor(data['seq'])\n",
    "        self.data_label = data['expression']\n",
    "\n",
    "        x = F.one_hot(self.data.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "        self.data2 = x.transpose(1,2).numpy()\n",
    "\n",
    "        print('trn_data_shape : ',self.data2.shape)\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data2[index]), self.data_label[index], torch.FloatTensor(self.seq[index]) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class test_loader(object):\n",
    "    def __init__(self,args):\n",
    "        lines = open(\"/Data1/PGE/torch_ti/filtered_test_data_with_MAUDE_expression.txt\", \"r\").read().splitlines()\n",
    "        data = [x.split('\\t')[0] for x in lines]\n",
    "        data_label = [x.split('\\t')[1] for x in lines]\n",
    "        df = pd.DataFrame()\n",
    "        df['dna'] = data\n",
    "        df['expression'] = data_label\n",
    "        df['dna'] = df['dna'].astype('string')\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        print('number of unique sequences in the first {} positions: {}'.format(args['head_len'], len(df['dna'].str[:args['head_len']].unique())))\n",
    "        print('number of unique sequences in the last {} positions: {}'.format(args['tail_len'], len(df['dna'].str[-args['tail_len']:].unique())))\n",
    "        df['dna'] = df['dna'].str[args['head_len']:]\n",
    "        df['dna'] = df['dna'].str[:-args['tail_len']]\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        assert all(df['len'] <= args['max_width'])\n",
    "        \n",
    "        df['dna'] = df['dna'].str.pad(width = args['max_width'], side = 'both', fillchar = 'N')\n",
    "        df['dna'] = df['dna'] + df['dna'].apply(lambda x: str(Seq(x).reverse_complement())).astype('string')\n",
    "        \n",
    "        input_dim = int(6) # A,C,G,T,N,M\n",
    "        n_positions = int(args['max_width'] * 2)\n",
    "        self.dna = np.empty((0, n_positions), np.uint8)\n",
    "        for x in np.array_split(df['dna'], 10): # split data into chunks\n",
    "            y = np.array(x.apply(list))\n",
    "            y = np.vstack(y)\n",
    "            y = np.vectorize(ARGS['alphabets'].get)(y)\n",
    "            y = y.astype(np.uint8)\n",
    "            print(y.shape)\n",
    "            self.dna = np.append(self.dna, y, axis = 0)\n",
    "        print(self.dna.shape)\n",
    "        self.expression = df['expression'].astype('float32').to_numpy()\n",
    "        expression_std = np.std(self.expression)\n",
    "        expression_mean = np.mean(self.expression)\n",
    "        self.expression = (self.expression - expression_mean) / expression_std\n",
    "        \n",
    "        print(self.expression.shape)\n",
    "        \n",
    "        \n",
    "        self.kmer = 10\n",
    "        self.strides = 1\n",
    "        self.input_dim = 6\n",
    "        self.data = torch.tensor(self.dna)\n",
    "        \n",
    "        x = F.one_hot(self.data.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "        self.data2 = x.transpose(1,2).numpy()  # output = (b,embed,seq)\n",
    "        \n",
    "        print('test_data_shape : ',self.data2.shape)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data2[index]), self.expression[index], torch.FloatTensor(self.dna[index]) \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5LDHmegztZL"
   },
   "outputs": [],
   "source": [
    "class train_loader(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data['seq']\n",
    "        self.data_label = data['expression']\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data[index]), self.data_label[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class test_loader(object):\n",
    "    def __init__(self,args):\n",
    "        lines = open(\"/Data1/PGE/torch_ti/filtered_test_data_with_MAUDE_expression.txt\", \"r\").read().splitlines()\n",
    "        data = [x.split('\\t')[0] for x in lines]\n",
    "        data_label = [x.split('\\t')[1] for x in lines]\n",
    "        df = pd.DataFrame()\n",
    "        df['dna'] = data\n",
    "        df['expression'] = data_label\n",
    "        df['dna'] = df['dna'].astype('string')\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        print('number of unique sequences in the first {} positions: {}'.format(args['head_len'], len(df['dna'].str[:args['head_len']].unique())))\n",
    "        print('number of unique sequences in the last {} positions: {}'.format(args['tail_len'], len(df['dna'].str[-args['tail_len']:].unique())))\n",
    "        df['dna'] = df['dna'].str[args['head_len']:]\n",
    "        df['dna'] = df['dna'].str[:-args['tail_len']]\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        assert all(df['len'] <= args['max_width'])\n",
    "        \n",
    "        df['dna'] = df['dna'].str.pad(width = args['max_width'], side = 'both', fillchar = 'N')\n",
    "        df['dna'] = df['dna'] + df['dna'].apply(lambda x: str(Seq(x).reverse_complement())).astype('string')\n",
    "        \n",
    "        input_dim = int(6) # A,C,G,T,N,M\n",
    "        n_positions = int(args['max_width'] * 2)\n",
    "        self.dna = np.empty((0, n_positions), np.uint8)\n",
    "        for x in np.array_split(df['dna'], 10): # split data into chunks\n",
    "            y = np.array(x.apply(list))\n",
    "            y = np.vstack(y)\n",
    "            y = np.vectorize(ARGS['alphabets'].get)(y)\n",
    "            y = y.astype(np.uint8)\n",
    "            print(y.shape)\n",
    "            self.dna = np.append(self.dna, y, axis = 0)\n",
    "        print(self.dna.shape)\n",
    "        self.expression = df['expression'].astype('float32').to_numpy()\n",
    "        expression_std = np.std(self.expression)\n",
    "        expression_mean = np.mean(self.expression)\n",
    "        self.expression = (self.expression - expression_mean) / expression_std\n",
    "        \n",
    "        print(self.expression.shape)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.dna[index]), self.expression[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KwOLqNOwU3U"
   },
   "source": [
    "## The regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, strides = 2, ratio = 0.2, ff_mult = 4, use_bias = False, num_projectors = 8):\n",
    "        super(FirstLayer_Block, self).__init__()\n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.input_dim = input_dim\n",
    "        self.kmer = kmer\n",
    "        self.strides = strides\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(self.n_positions, embedding_dim)\n",
    "        self.strand_embedding = nn.Embedding(2, embedding_dim) # plus/minus strands\n",
    "        self.expression_embedding = nn.Linear(1,embedding_dim)\n",
    "        self.kmer_dense = nn.Linear(input_dim*self.kmer,embedding_dim)\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq, embed*kmer)\n",
    "        print('input_shape : ',x.shape)\n",
    "        \n",
    "        print('unfold shape : ',x.shape)\n",
    "        x = self.kmer_dense(x)\n",
    "\n",
    "        pos = torch.arange(start=0, end = self.n_positions, step=1).cuda()\n",
    "        pos = pos.unsqueeze(0)\n",
    "        pos = self.pos_embedding(pos.long())\n",
    "\n",
    "        strand = torch.tensor(np.repeat([0,1], repeats = int(self.n_positions / 2))).cuda()\n",
    "        strand = strand.unsqueeze(0)\n",
    "        strand = self.strand_embedding(strand.long())\n",
    "\n",
    "        x = x + pos + strand  # 채널 dim=2\n",
    "\n",
    "        expression = torch.zeros((batch_size, self.num_projectors, 1)).cuda()\n",
    "        expression = self.expression_embedding(expression.float())\n",
    "\n",
    "        x = torch.cat([expression, x], dim = 1)\n",
    "        x = x.transpose(1,2)\n",
    "        print('first_output_shape : ',x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, strides = 2, ratio = 0.2, ff_mult = 4, use_bias = False, num_projectors = 8):\n",
    "        super(FirstLayer_Block, self).__init__()\n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.input_dim = input_dim\n",
    "        self.kmer = kmer\n",
    "        self.strides = strides\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        \n",
    "        self.pos_embedding = nn.Embedding(self.n_positions, embedding_dim)\n",
    "        self.strand_embedding = nn.Embedding(2, embedding_dim) # plus/minus strands\n",
    "        self.expression_embedding = nn.Linear(1,embedding_dim)\n",
    "        self.kmer_dense = nn.Linear(input_dim*self.kmer,embedding_dim)\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq)\n",
    "        print('input_shape : ',x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = F.one_hot(x.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "\n",
    "        x = x.unsqueeze(2)  # b,seq,em,1\n",
    "        x_shape = x.shape\n",
    "        fold_shape = x.unfold(1,self.kmer,self.strides).transpose(3,4).shape\n",
    "        div = x_shape[1] - fold_shape[1]\n",
    "        x = F.pad(x.unfold(1,self.kmer,self.strides).transpose(3,4),(0,0,0,0,0,0,0,div),'constant',0).reshape(x.shape[0],x.shape[1]//self.strides,x.shape[2],-1)\n",
    "        x = x.squeeze(2).float()\n",
    "        print('unfold shape : ',x.shape)\n",
    "        x = self.kmer_dense(x)\n",
    "\n",
    "        pos = torch.arange(start=0, end = self.n_positions, step=1).cuda()\n",
    "        pos = pos.unsqueeze(0)\n",
    "        pos = self.pos_embedding(pos.long())\n",
    "\n",
    "        strand = torch.tensor(np.repeat([0,1], repeats = int(self.n_positions / 2))).cuda()\n",
    "        strand = strand.unsqueeze(0)\n",
    "        strand = self.strand_embedding(strand.long())\n",
    "\n",
    "        x = x + pos + strand  # 채널 dim=2\n",
    "\n",
    "        expression = torch.zeros((batch_size, self.num_projectors, 1)).cuda()\n",
    "        expression = self.expression_embedding(expression.float())\n",
    "\n",
    "        x = torch.cat([expression, x], dim = 1)\n",
    "        x = x.transpose(1,2)\n",
    "        print('first_output_shape : ',x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Core_Block(nn.Module):\n",
    "    def __init__(self, embedding_dim = 32, input_dim = 5, n_blocks = 4, \n",
    "               kernel_size =15, rate = 0.2, num_heads = 4):\n",
    "        super(Core_Block, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.blocks = nn.ModuleList([ConformerSASwiGLULayer(embedding_dim = embedding_dim,\n",
    "                                    kernel_size = kernel_size, rate = rate, num_heads = num_heads) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x): \n",
    "        print('core_input_shape : ',x.shape)\n",
    "        for i in range(self.n_blocks) :\n",
    "            x = self.blocks[i](x)\n",
    "        print('core_output_shape : ',x.transpose(1,2).shape)\n",
    "        return x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, embedding_dim = 32, input_dim = 5, rate = 0.2,\n",
    "                 strides = 2, use_bias = False, num_projectors = 8):\n",
    "        super(FinalLayer_Block, self).__init__()\n",
    "        \n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.expression_dense = nn.Linear(embedding_dim,1)\n",
    "        self.nucleotide_dense = nn.Linear(embedding_dim,input_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "        print('final_input_shape : ',x.shape)\n",
    "        expression = x[:,:self.num_projectors,:]\n",
    "        x = x[:, -self.n_positions:, :]\n",
    "\n",
    "        expression = self.dropout(expression)\n",
    "        expression = self.expression_dense(expression)\n",
    "        expression = torch.mean(expression, 1)\n",
    "\n",
    "        x = self.nucleotide_dense(x)\n",
    "        print('final_seq_reproduce_shape : ',x.transpose(1,2).shape)\n",
    "        print('final_output_shape : ',expression.shape)\n",
    "        return expression, x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, n_blocks = 4, \n",
    "               kernel_size =15, rate = 0.2, strides = 2, ratio = 0.2, num_heads = 4, ff_mult = 4, \n",
    "               use_bias = False, num_projectors = 8):\n",
    "        super(Regressor, self).__init__()\n",
    "        \n",
    "        self.masking = SequenceMaskLayer(n_positions = n_positions, ratio = ratio)\n",
    "        \n",
    "        self.first_block = FirstLayer_Block(n_positions, kmer, embedding_dim, input_dim, strides, \n",
    "                                            ratio, ff_mult, use_bias, num_projectors)\n",
    "        self.core_block = Core_Block(embedding_dim, input_dim, n_blocks, \n",
    "                                     kernel_size, rate, num_heads)\n",
    "        self.final_block = FinalLayer_Block(n_positions, embedding_dim, input_dim, rate,\n",
    "                                       strides, use_bias, num_projectors)\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq)\n",
    "\n",
    "        first_out = self.first_block(x)\n",
    "        core_out = self.core_block(first_out)\n",
    "        expression, seq_out = self.final_block(core_out)\n",
    "\n",
    "        return expression, seq_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-jAAiPA0oMS"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from prixfixe.unlockdna import (UnlockDNA_CoreBlock,\n",
    "                      UnlockDNA_FirstLayersBlock,\n",
    "                      UnlockDNA_FinalLayersBlock)\n",
    "\n",
    "from prixfixe.prixfixe import PrixFixeNet\n",
    "\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "\tdef __init__(self, args,**kwargs):\n",
    "\t\tsuper(RegressorModel, self).__init__()\n",
    "\t\t## regressor\n",
    "\t\tself.arg = args\n",
    "\t\tfirst = UnlockDNA_FirstLayersBlock(in_channels=6,\n",
    "                                   out_channels=512, \n",
    "                                   seqsize=200)\n",
    "\t\tcore = UnlockDNA_CoreBlock(in_channels=512,\n",
    "                         out_channels =512,\n",
    "                         seqsize=232)        \n",
    "\t\tfinal = UnlockDNA_FinalLayersBlock(in_channels=512, \n",
    "                                 seqsize=232)       \n",
    "\t\t        \n",
    "\t\tself.regressor = PrixFixeNet(first=first,core=core,final=final).cuda()ß\n",
    "\t\tself.mse_loss = nn.MSELoss(reduction='none').cuda()\n",
    "\t\tself.scc_loss = nn.CrossEntropyLoss( reduction='none').cuda()\n",
    "\t\tself.optim           = torch.optim.Adam(self.regressor.parameters(), lr = args['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "\t\tself.scheduler       = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=ARGS['max_lr'],pct_start = 0.04, \n",
    "                                                                   steps_per_epoch=ARGS['warmup_steps'], epochs=ARGS['epochs']+5,anneal_strategy='cos')ß\n",
    "\t\tprint(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number(백만) = %.2f\"%(sum(param.numel() for param in self.regressor.parameters()) / 1024 / 1024))\n",
    "\n",
    "\tdef train_network(self, epoch, loader):\n",
    "\t\tself.train()\n",
    "\t\t## Update the learning rate based on the current epoch\n",
    "\t\tif epoch > 0 :\n",
    "\t\t\tself.scheduler.step((epoch - 1)*int(n_train/self.arg['batch_size']))\n",
    "\t\t\tprint('LR : ',self.scheduler.get_last_lr()[0])\n",
    "\t\tindex, loss = 0, 0\n",
    "\t\tfor num, (data, labels, sequence) in tqdm(enumerate(loader, start = 1)):\n",
    "\n",
    "\t\t\tself.zero_grad()\n",
    "\t\t\tseq, mask = self.regressor.masking(sequence)\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\t\texpression, seq_pred = self.regressor.forward(data.cuda()) \n",
    "\t\t\tloss_expression = self.mse_loss(labels.to(torch.float32), expression.squeeze(1).to(torch.float32))\n",
    "\t\t\tloss_seq = mask.cuda() * self.scc_loss(seq_pred,sequence.long().cuda()) #.long()\n",
    "\t\t\tloss_seq = torch.sum(loss_seq) / (torch.sum(mask.cuda()) + 1)\n",
    "\t\t\tnloss = (loss_expression.to(torch.float32) + loss_seq.to(torch.float32)).mean().to(torch.float32)\n",
    "\t\t\t\n",
    "\t\t\tnloss.backward()\n",
    "\t\t\tself.optim.step()\n",
    "\t\t\tself.scheduler.step()\n",
    "\t\t\tlr = self.scheduler.get_last_lr()[0]\n",
    "\t\t\tindex += len(labels)\n",
    "\t\t\tloss += nloss.detach().cpu().numpy()\n",
    "\t\t\tif num % 500 == 0 :\n",
    "\t\t\t\tsys.stderr.write(time.strftime(\"%m-%d %H:%M:%S\") + \\\n",
    "\t\t\t\t\" [%2d] Lr: %5f, Training: %.2f%%, \"    %(epoch, lr, 100 * (num / loader.__len__())) + \\\n",
    "\t\t\t\t\" Loss: %.5f \\r\"        %(loss/(num)))\n",
    "\t\t\t\tsys.stderr.flush()\n",
    "\t\t\t\tsys.stdout.write(\"\\n\")\n",
    "\n",
    "\t\treturn loss/num, lr \n",
    "\n",
    "\tdef eval_network(self, loader):\n",
    "\t\tself.eval()\n",
    "\t\texp = []\n",
    "\t\treal_exp = []\n",
    "\t\tfor idx, (data,labels,_) in tqdm(enumerate(loader)):\n",
    "\t\t\tdata_1 = torch.FloatTensor(data).cuda()\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\texpression, _ = self.regressor.forward(data_1)\n",
    "\t\t\tif len(expression.shape) > 1 :\n",
    "\t\t\t\texpression = expression.reshape(-1)\n",
    "\t\t\t\texpression = expression.detach().cpu().numpy()\n",
    "\t\t\t\tlabels = labels.detach().cpu().numpy() \n",
    "\t\t\texp.append(expression)\n",
    "\t\t\treal_exp.append(labels)\n",
    "\t\t\t\n",
    "\t\t# Coumpute Metric\n",
    "\t\texp = np.array(exp).reshape(-1)\n",
    "\t\treal_exp = np.array(real_exp).reshape(-1)\n",
    "\t\tPR = pearson_r(exp, real_exp)\n",
    "\n",
    "\t\treturn PR\n",
    "\n",
    "\tdef save_parameters(self, path):\n",
    "\t\ttorch.save(self.state_dict(), path)\n",
    "\n",
    "\tdef load_parameters(self, path):\n",
    "\t\tself_state = self.state_dict()\n",
    "\t\tloaded_state = torch.load(path)\n",
    "\t\tfor name, param in loaded_state.items():\n",
    "\t\t\torigname = name\n",
    "\t\t\tif name not in self_state:\n",
    "\t\t\t\tname = name.replace(\"module.\", \"\")\n",
    "\t\t\t\tif name not in self_state:\n",
    "\t\t\t\t\tprint(\"%s is not in the model.\"%origname)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tif self_state[name].size() != loaded_state[origname].size():\n",
    "\t\t\t\tprint(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tself_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOtXgSxYvy0F"
   },
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGLEOzKhpPE2"
   },
   "source": [
    "### Epoch 1 \n",
    "\n",
    "initial epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VpPeGE-OeX-",
    "outputId": "0cf7da67-67f8-4b6d-b151-a0c481d9fcb4"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This is the main code of the ECAPATDNN project, to define the parameters and build the construction\n",
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import argparse, glob, os, torch, warnings, time\n",
    "\n",
    "\n",
    "model_save_path = \"exps/model_reproducing_3module\"\n",
    "score_save_path = \"exps/score_reproducing_3module.txt\"\n",
    "os.makedirs(model_save_path,exist_ok = True)\n",
    "\n",
    "device = ARGS['device']\n",
    "torch.cuda.set_device(device)\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "\n",
    "\n",
    "## Define the data loader\n",
    "trainloader = train_loader(train_data)\n",
    "trainLoader = torch.utils.data.DataLoader(trainloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "valloader = train_loader(val_data)\n",
    "valLoader = torch.utils.data.DataLoader(valloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "testloader = test_loader(ARGS)\n",
    "testLoader = torch.utils.data.DataLoader(testloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "\n",
    "## Search for the exist models\n",
    "modelfiles = glob.glob('%s/model_0*.model'%model_save_path)\n",
    "modelfiles.sort()\n",
    "\n",
    "## Otherwise, system will try to start from the saved model&epoch\n",
    "if len(modelfiles) >= 1:\n",
    "\tprint(\"Model %s loaded from previous state!\"%modelfiles[-1])\n",
    "\tepoch = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][6:]) + 1\n",
    "\ts = RegressorModel(ARGS)\n",
    "\ts.load_parameters(modelfiles[-1])\n",
    "\teval_pr_ = s.eval_network(testLoader)\n",
    "\tprint('Previous Eval Pearson_R : ',eval_pr_)\n",
    "## Otherwise, system will train from scratch\n",
    "else:\n",
    "\tepoch = 1\n",
    "\ts = RegressorModel(ARGS)\n",
    "\n",
    "pr = []\n",
    "eval_pr = []\n",
    "score_file = open(score_save_path, \"a+\")\n",
    "\n",
    "while(1):\n",
    "\t## Training for one epoch\n",
    "\tloss, lr = s.train_network(epoch = epoch, loader = trainLoader)\n",
    "\n",
    "\t## Evaluation every [test_step] epochs\n",
    "\tif epoch % 1 == 0:\n",
    "\t\ts.save_parameters(model_save_path + \"/model_%04d.model\"%epoch)\n",
    "\t\tpr.append(s.eval_network(valLoader))\n",
    "\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, Pearson_R %2.2f%%, bestPearson_R %2.2f%%\"%(epoch, pr[-1], max(pr)))\n",
    "\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, Pearson_R %2.2f%%, bestPearson_R %2.2f%%\\n\"%(epoch, lr, loss, pr[-1], max(pr)))\n",
    "\t\tscore_file.flush()\n",
    "\t\tif pr[-1] == max(pr) :\n",
    "\t\t\ts.save_parameters(model_save_path + \"/model_best.model\")\n",
    "\t\t\teval_pr.append(s.eval_network(testLoader))\n",
    "\t\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, Eval_Pearson_R %2.2f%%, Best_Eval_Pearson_R %2.2f%%\"%(epoch, eval_pr[-1], max(eval_pr)))\n",
    "\t\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, Pearson_R %2.2f%%, Best_Eval_Pearson_R %2.2f%%\\n\"%(epoch, lr, loss, eval_pr[-1], max(eval_pr)))\n",
    "\t\t\tscore_file.flush()\n",
    "\n",
    "\tif epoch >= ARGS['epochs']:\n",
    "\t\tquit()\n",
    "\n",
    "\tepoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp22H950YtNt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19E3HT1ESJ4A"
   },
   "source": [
    "## Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "53wVoagb4tD8",
    "outputId": "74fa0970-dc30-492f-9cc9-76ce90418ec9"
   },
   "outputs": [],
   "source": [
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFWCf4x-5bTE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
