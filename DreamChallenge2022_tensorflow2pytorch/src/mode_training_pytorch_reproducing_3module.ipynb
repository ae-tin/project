{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0jDe-m_5aq4"
   },
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wH9oF2RleEYl"
   },
   "source": [
    "## Load Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XamZpd5EeGHR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import session_info\n",
    "import pdb\n",
    "from sklearn.metrics import r2_score\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from Bio.Seq import Seq\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import socket\n",
    "import time\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import random as python_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "w9lA5-iVI0d6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import math\n",
    "import pickle as pk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl2lXOAv9ljx"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "D0pjVLfcfMl7"
   },
   "outputs": [],
   "source": [
    "RUNTIME = 'none'\n",
    "ARGS = {\n",
    "  'model_id' : 'm20220727e',\n",
    "  'global_seed' : 123,\n",
    "  'shuffle_size' : 1000,\n",
    "  'max_width' : 100,\n",
    "  'head_len' : 17,\n",
    "  'tail_len' : 13,\n",
    "  'pct_ds' : 1, # % of total data for training/testing,\n",
    "  'train_split' : 0.95,\n",
    "  'alphabets' : {'A' : 0, 'C' : 1, 'G' : 2, 'T' : 3, 'N' : 4, 'M' : 5},\n",
    "  'initial_lr' : 1e-15,\n",
    "  'max_lr' : 3e-4,\n",
    "  'initial_epoch': 0,\n",
    "  'epochs' : 20,\n",
    "  'batch_size' : 512,\n",
    "  'dropout_rate' : 0.1,\n",
    "  'kmer': 10,\n",
    "  'strides' : 1,\n",
    "  'embedding_dim' : 512,\n",
    "  'num_heads' : 8,\n",
    "  'ff_mult' : 4,\n",
    "  'num_projectors' : 32,\n",
    "  'n_blocks_regressor' : 4,\n",
    "  'warmup_steps' : 12500, # ~ 1 epoch\n",
    "  'mask_ratio' : 0.05,\n",
    "  'remote_sample_submission_file' : 'https://raw.githubusercontent.com/de-Boer-Lab/DREAM-2022/main/sample_submission.json',\n",
    "  'eval' : False,\n",
    "  'device':'cuda:1'\n",
    "}\n",
    "if RUNTIME == 'msi':\n",
    "  ARGS['remote_data_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/training_data/pct_ds=' + str(ARGS['pct_ds']) + '/'\n",
    "  ARGS['local_data_dir'] = re.sub('https://', './', ARGS['remote_data_dir'])\n",
    "  ARGS['remote_checkpoint_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/notebooks_msi/' + ARGS['model_id'] + '/tf_ckpts/'\n",
    "  ARGS['remote_log_dir'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/notebooks_msi/' + ARGS['model_id'] + '/log/'\n",
    "  ARGS['local_checkpoint_dir'] = re.sub('https://', './', ARGS['remote_checkpoint_dir'])\n",
    "  ARGS['local_log_dir'] = re.sub('https://', './', ARGS['remote_log_dir'])\n",
    "  ARGS['remote_test_data'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/test_sequences.txt.gz'\n",
    "  ARGS['local_test_data'] = re.sub('https://', './', ARGS['remote_test_data'])\n",
    "  ARGS['local_sample_submission_file'] = re.sub('https://', './', ARGS['remote_sample_submission_file'])\n",
    "  ARGS['remote_prediction_file'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/predictions/' + ARGS['model_id'] + '/pred.json'\n",
    "  ARGS['local_prediction_file'] = re.sub('https://', './', ARGS['remote_prediction_file'])\n",
    "  ARGS['s3_prediction_file'] = re.sub('https://s3.msi.umn.edu', 's3://', ARGS['remote_prediction_file'])\n",
    "  ARGS['remote_prediction_tsv_file'] = 'https://s3.msi.umn.edu/gongx030/projects/dream_PGE/predictions/' + ARGS['model_id'] + '/pred.tsv'\n",
    "  ARGS['local_prediction_tsv_file'] = re.sub('https://', './', ARGS['remote_prediction_tsv_file'])\n",
    "  ARGS['s3_prediction_tsv_file'] = re.sub('https://s3.msi.umn.edu', 's3://', ARGS['remote_prediction_tsv_file'])\n",
    "else:\n",
    "  ARGS['local_data_dir'] = '/content/drive/MyDrive/training_data/pct_ds=' + str(ARGS['pct_ds']) + '/'\n",
    "  ARGS['local_checkpoint_dir'] = '/content/drive/MyDrive/' + ARGS['model_id'] + '/tf_ckpts/'\n",
    "  ARGS['local_log_dir'] = '/content/drive/MyDrive/' + ARGS['model_id'] + '/log/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARGS['local_data_dir'] = '/Data1/PGE/torch_ti/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ZhZvwWifIpzY"
   },
   "outputs": [],
   "source": [
    "with open(ARGS['local_data_dir']+\"data.pk\",\"rb\") as fr:\n",
    "    data = pk.load(fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5i5XrE1H2Iwq"
   },
   "source": [
    "### Set seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5LTFxNSIWQID"
   },
   "outputs": [],
   "source": [
    "np.random.seed(ARGS['global_seed'])\n",
    "torch.manual_seed(ARGS['global_seed'])\n",
    "python_random.seed(ARGS['global_seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oPZLHuW06Tcm"
   },
   "source": [
    "### pearson_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bDou1j1A6S2j"
   },
   "outputs": [],
   "source": [
    "def pearson_r(x, y):\n",
    "    x = torch.tensor(x,dtype=torch.float32)\n",
    "    y = torch.tensor(y,dtype=torch.float32)\n",
    "    mx = torch.mean(x, axis = 0, keepdims = True)\n",
    "    my = torch.mean(y, axis = 0, keepdims = True)\n",
    "    xm = x - mx\n",
    "    ym = y - my\n",
    "    t1_norm = F.normalize(xm, p=2, dim=0)\n",
    "    t2_norm = F.normalize(ym, p=2, dim=0)\n",
    "    return torch.sum(torch.mul(t1_norm, t2_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E4kvC_v6oYG",
    "outputId": "53ce49be-7138-490b-ba8f-85a92dc3592e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pearson r (stats.pearsonr): -0.09270195576139686\n",
      "pearson r (pearson_r): -0.09270194172859192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4054/3244679858.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x,dtype=torch.float32)\n",
      "/tmp/ipykernel_4054/3244679858.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y,dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100)\n",
    "y = np.random.rand(100)\n",
    "print('pearson r (stats.pearsonr): {}'.format(stats.pearsonr(x, y)[0]))\n",
    "print('pearson r (pearson_r): {}'.format(pearson_r(torch.unsqueeze(torch.Tensor(x),1), torch.unsqueeze(torch.Tensor(y),1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrQ9T4-oSk8X"
   },
   "source": [
    "### GLULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "oMX5LC1aSkRK"
   },
   "outputs": [],
   "source": [
    "class GLULayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(GLULayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out,gate = torch.chunk(x, 2, dim = self.dim)\n",
    "        return out * self.sig(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ivpHjDlQSxUU",
    "outputId": "6f85a037-81fc-4924-cbb0-074aa9e0d808"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = GLULayer(dim = 1)\n",
    "x = torch.randn(3,6,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHdNKhzVvsMf"
   },
   "source": [
    "### SwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WeSyNETvvuex"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SwiGLULayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(SwiGLULayer, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.swish = nn.SiLU() # same as swish\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, gate = torch.chunk(x, 2, dim = self.dim)\n",
    "        return out * self.swish(gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JuwI-YXjvuex",
    "outputId": "7e3be8c3-6e7a-4886-d85d-26c9c763b953"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = SwiGLULayer(dim = 1)\n",
    "x = torch.randn(3,6,10)\n",
    "layer(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7BVgkw-w4UO"
   },
   "source": [
    "### FeedForwardSwiGLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "eVdwwIxbw4UP"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForwardSwiGLU(nn.Module):\n",
    "    def __init__(self, embedding_dim, mult=4, rate = 0.0, use_bias = False):\n",
    "        super(FeedForwardSwiGLU, self).__init__()\n",
    "        swiglu_out = int(embedding_dim * mult/2)\n",
    "        self.layernorm = nn.LayerNorm(embedding_dim,eps = 1e-6)\n",
    "        self.linear1 = nn.Linear(embedding_dim,embedding_dim * mult, bias = use_bias)\n",
    "        self.swiglulayer = SwiGLULayer(dim = 1)\n",
    "        self.drop = nn.Dropout(rate)\n",
    "        self.linear2 = nn.Linear(swiglu_out,embedding_dim, bias = use_bias)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.layernorm(inputs.transpose(1,2)) # 차원바뀌고 채널 dim=2\n",
    "        x = self.linear1(x) \n",
    "        x = self.swiglulayer(x.transpose(1,2)) # 또 차원 바뀌고 채널 dim =1\n",
    "        x = self.drop(x)\n",
    "        x = self.linear2(x.transpose(1,2)) # 차원 바뀌고 채널 dim=2\n",
    "        out = self.drop(x.transpose(1,2)) # 차원 또 바뀌고 채널 dim =1\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdHfQ6HHw4UP",
    "outputId": "966a8466-6c37-415f-cb16-2e13189dbb78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 10])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForwardSwiGLU(embedding_dim = 5, mult = 4)\n",
    "x = torch.randn(3,5,10)\n",
    "print(ffn(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rke3X-N6-n9S"
   },
   "source": [
    "### CustomSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "X73TEopBGYov"
   },
   "outputs": [],
   "source": [
    "n_train = 6400689\n",
    "model = FeedForwardSwiGLU(embedding_dim = 5, mult = 4)\n",
    "optim           = torch.optim.Adam(model.parameters(), lr = ARGS['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "scheduler       = torch.optim.lr_scheduler.OneCycleLR(optim, max_lr=ARGS['max_lr'],pct_start = 0.05, \n",
    "                                                                   steps_per_epoch=int(n_train/ARGS['batch_size'])+1, epochs=ARGS['epochs'],anneal_strategy='cos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "r-w_RbKWIXeM",
    "outputId": "b1995ed8-b95f-4da8-9b01-707fb00c2772"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'OneCycleLR Scheduler')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAEWCAYAAABWn/G6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAA63ElEQVR4nO3dd5xU1dnA8d+zs72wwLL0Xiw0BZYaoiSaiCViQQUVGyC2GF9jLG/K62uSN4mJvUuxoQLBFOw9BkXKIoKAIk3pnWUL2/d5/7hnzbhumYWdubuzz/fzmQ8z59577nN2lnn23HvmHFFVjDHGmEiJ8TsAY4wxzYslHmOMMRFliccYY0xEWeIxxhgTUZZ4jDHGRJQlHmOMMRFliceYMBKRO0Vktt9xhEpEVER6N1Bd/xKRKQ29r2n6LPGYJkNErhCRz0TksIjsEpHHRKRlA9Z/jIj8VUT2icghEVklIjeLSKChzlHH+av98BWR7i4h5LvHVyJyex11jRORT0Uk17XnPRHpEb7ojQmdJR7TJIjIz4E/Ab8A0oERQDfgbRGJb4D6ewFLgK3AAFVNBy4AsoC0o62/gbRU1VRgPPBrEflRdTu5HsuzwM/xflY9gEeA8kgFGikiEut3DKb+LPGYRk9EWgD/C/xUVd9Q1VJV/Qq4EOgOXOr2u1NE5onIsyKSJyJrRCQrqJ6OIvKSiOwVkc0icmPQaf4XWKSqN6vqTgBVXaeqF6tqjoi8KiI/rRLXKhE51z3vJyJvi8gBEdktIv9dQ1tGiMgiEckRkZUiMqa+Pw9VzQbWACfWsMuJwGZVfVc9ear6kqpucTEEROS/RWSj+zktF5EuQcefKiLrXYyPiIgExX+ViHwuIgdF5E0R6Ra07Uci8oXrLT4MBB/3rUuOQb24ahNHHedREbleRNYD6+vxozONhCUe0xSMAhKBvwUXqmo+8BoQ/Jf/2cAcoCWwAHgYQERigJeBlUAn4BTgJhE5zR13KjC/lhiewSU4V98Jrp5XRSQNeAd4A+gI9AberVqBiHQCXgV+B7QGbgFeEpHMOtpftZ4RQH9gQw27fAIcJyL3icgPRCS1yvabgYnAGUAL4CrgcND2s4ChwEC85H6aO+844L+B84BMYCHwotvWBu/9+RXQBtgIfK8+7QpqX43nCXIOMBzoeyTnMP6yxGOagjbAPlUtq2bbTre90oeq+pqqlgPPASe48qFApqrepaolqroJmA5McNszXF01WQAcIyJ93OtJwFxVLcH7oN6lqveoapHrYSyppo5LgddcfBWq+jaQjZcAQrFPRAqBj4FHgX9Ut5Nr2xi8xDjPHfd0UAKaAvzK9ehUVVeq6v6gKv6oqjmuh/Q+/+lZXQP8QVU/d+/F/wEnut7IGcAaVZ2vqqXA/cCuENtVVW3nqfQHVT2gqoVHeA7jI0s8pinYB7Sp4bJMB7e9UvCH3WEg0R3XDejoLh/liEgO3l/V7dy++11d1VLVImAucKnrPU3ES2wAXfD+wq9LN+CCKjGMru28VbQBUvHu3YwB4mqJd7GqXqiqmcD3gZOAX4YYb9WfYWXC6gY8EBT7AbzLaZ3wenpbg86vwa/rqbbzVDrSuk0jYInHNAUfA8V4l16+4f6CP51qLmtVYyvefY+WQY80Va3sbbwDnF9HHc8Al+Bdpjusqh8H1d0zxBieqxJDiqr+MYRjAVDVclW9FygCrgvxmGV4l8H6B8XRK9RzBtkKTKsSf5KqLsLrLX5zn8jdFwq+b1QAJAe9bn+E5/mmWUcQv2kkLPGYRk9VD+Hd/H9IRMaKSJyIdMe7jLSN//Q8arMUyBOR20Qkyd1g7y8iQ932/wFGicifRaQ9eKPDRGS2uCHbLtFUAPdUOecrQAcRuUlEEkQkTUSGVxPDbOAnInKaO3+iiIwRkc5B+8S68spHTb2aPwK3ikhi1Q0iMlpEpopIW/f6OLx7X4vdLjOA34pIH/EMFJGMun6AwOPAHSLSz9WbLiIXuG2vAv1E5DzXw7yRbyeXT4GTRKSriKQDdxzheUwUsMRjmgRVvRvv0thfgFz+M/T5FFUtDuH4crx7MScCm/Euz83AG26Mqm4ERuKNklsjIoeAl/DuweQFVfUsMAAviVTWnYc3wOEneJep1gM/qCaGrUDljfO9Lv5f8O3/h48BhUGPp2po0qvAQWBqNdty8BLNZyKSjzfo4e/A3W77vXhJ+y28n+VMIKmG8wTH/3e8Ie1zRCQXWI3X40RV9+ENP/8j3mXLPsBHQce+jXepchWwHC9Z1/s8JjqILQRnTOhE5DLgalUd7XcsxjRV1uMxJkQikox3X+VJv2MxpimzxGNMCNz3ffYCu4EXfA7HmCbNLrUZY4yJKOvxGGOMiSibYK8abdq00e7du/sdhjHGNCnLly/f5760XCtLPNXo3r072dnZfodhjDFNioh8Hcp+dqnNGGNMRFniMcYYE1GWeIwxxkSUJR5jjDERZYnHGGNMRIU18biZhNeJyAYRub2a7QkiMtdtX+JmHK7cdocrXxe0SmSNdYrITPGWEl4lIvMrF72q7RzGGGMiL2yJR0QCwCN4s8r2BSaKSNVlaicDB1W1N3Af3oy0uP0mAP2AscCjbhr52ur8L1U9QVUHAluAG2o7hzHGGH+E83s8w4ANbhleRGQO3pTwa4P2GQfc6Z7PBx52C0iNA+a46e43i8gGVx811amqua5M8KZ419rOoY1srqCi0nJmL/6avKIy4mNjSIwLkBQXIDEuhhaJcWSkxtMmNYHMtAQS4wJ+h2uMMUcsnImnE99ennYbUHVxrG/2UdUytwZKhitfXOXYymVva6xTRJ7CW/t9Ld7ywLWdI3i5ZETkauBqgK5du9ajmQ3jvne+5IkPNoW0b1pCLF1aJ9O9TTJdW6fQPSOZPu3SOL5DGsnx9p1gY0zjFlWfUqp6pbsc9xBwETUvolXdsU/iprvPysqKaG9IVZmfvY2x/drz2KWDKS6roKi0nKLSCgpLy8ktLGV/QTH78krYV1DMntxivt5fwBc783h77W5Ky71wRaBnmxT6dkxnYKd0srq3on+ndOICNobEGNN4hDPxbOfba653dmXV7bPNLZebjrd6YW3H1lqnqpa7S3C34iWems7RaGzYk8/+ghJ+cFwmIkJiXCDky2nlFcqOnEI+35nL2p25rNmRyydfH+TllTsASIoLMLhbS4Z1z+D7x7ThhM4tCcRIOJtjjDG1CmfiWQb0EZEeeB/+E4CLq+yzALgc+BgYD7ynqioiC4AXROReoCPeMrpLAamuTndfp5eqbnDPzwa+qO0c4Wr0kcj++iAAw3qEsuz9twVihC6tk+nSOpkf9/vPEvd78opYtvkgy746wJLNB7j/3S+5750vaZ0Sz8nHZDLm2EzGHNOW9OS4BmuHMcaEImyJx91PuQF4EwgAs1R1jYjcBWSr6gK8td6fc4MHDuAlEtx+8/Du1ZQB16tqOUANdcYAz4hIC7zktBK41oVS7Tkak/W780mKC9CtdXKD1dk2LZEzB3bgzIEdAMg5XMK/1+/j/S/28MGXe/n7iu3Exgij+7ThJwM78qN+7WiRaEnIGBN+thBcNbKysjSSs1Nf8dRS9uQW89rPvh+R85VXKCu35fDmml28snIn23MKiY+NYcwxmZw3uDOnHN/W7gsZY+pNRJaralZd+0XV4IKmauPefE7s0ipi5wvECIO7tmJw11bcPvY4PtmSwyurdvDKqp28tXY3bVITGD+kMxOGdqF7m5SIxWWMaR4s8fisqLScbQcLOW9QZ1/OLyIM6daKId1a8cszjueDL/cyZ9lWpi/cxOMfbGRkzwyu+F53Tj2+nQ1KMMY0CEs8PtuRU4gqdG3A+ztHKjYQwynHt+OU49uxO7eI+cu38cKSLUx7bjndMpK5clR3LsjqQkqC/doYY46cXcj32a7cIgA6tEz0OZJva9ciket/0JsPfjGGRy8ZTEZKPHe+vJYRf3iXP7z+OXvziv0O0RjTRNmfrj7bdcglnvQknyOpXmwghjMGdOCMAR34ZMtBZn64men/3sQzi77ikuHdmHZST9q2aFxJ0xjTuFni8dlOl3jaN4EP78FdWzH44lZs2pvPw+9v4OlFXzF78ddMHNaVa8f0ol0TaIMxxn92qc1nu3OLSE+KIym+6Uz82TMzlXsvPJF3bz6ZcSd25LnFX3Pyn9/n7je+ILeo1O/wjDGNnCUen+08VESH9KbZU+jeJoW7x5/A+z8fw2n92vPovzZy8t3vM/PDzRSXlfsdnjGmkbLE47Ndh4po30QTT6WuGck8MGEQr/x0NP06pvPbV9Zy6r0f8PLKHdgXlI0xVVni8dmu3KImcX8nFP07pTN7ynCevWoYqQlx/PTFFUycvph1u/L8Ds0Y04hY4vFRRYVyoKCEjNR4v0NpUCcdk8krPx3N78/tzxe78jjjwYX878trOFRo93+MMZZ4fJVbVEp5hdI6JcHvUBpcIEa4ZHg33v/5GCYM7cLTi77ilHv+xfzl2+zymzHNnCUeH+0vKAEgIyW6ejzBWqXE8/tzB/DyDaPp2jqZW/66kkkzl7Jl/2G/QzPG+MQSj48OusTTOooTT6X+ndKZf80ofntOfz7dmsNp9/+bGQs3UV5hvR9jmhtLPD7a34wSD0BMjDBpRDfevvkkvtc7g9+9+jnnPfoRn+/M9Ts0Y0wEWeLx0YFmlngqdUhPYvplWTw0cRDbDhbyk4c+5OH31lNWXuF3aMaYCLDE46PmmnjAW47hJyd05J2bT+aMAR34y1tfcsETH/PVvgK/QzPGhJklHh/tzy8hJT5AYlzTmS6nobVKiefBiYN4cOIgNu7J5/QHFjJ78dc28s2YKGaJx0cHCoppHWXf4TlSZ5/Qkbf+62SyurfiV/9YzZVPL2OPWzLCGBNdLPH46MDhUlonW+Kp1D49kWevGsZd4/qxeNN+Tn9gIR98udfvsIwxDcwSj48OFBQ3y/s7tRERLhvZnVd+OprMtAQun7WUP77+BaU28MCYqGGJx0cH8kuictaChtC7bRr/uP57XDy8K49/sJGLnviYbQftS6fGRANLPD7KKSylZXKc32E0WolxAf7v3AE8NHEQX+7O54wHFvLmml1+h2WMOUqWeHxSWl7B4ZJy0pMs8dTlJyd05NUbR9MtI4Vpzy3n96+ute/8GNOEhTXxiMhYEVknIhtE5PZqtieIyFy3fYmIdA/adocrXycip9VVp4g878pXi8gsEYlz5WNE5JCIfOoevwlnm0OV62ZqtsQTmm4ZKcy/diSTRnRj+sLNXDpzCfvyi/0OyxhzBMKWeEQkADwCnA70BSaKSN8qu00GDqpqb+A+4E/u2L7ABKAfMBZ4VEQCddT5PHAcMABIAqYEnWehqp7oHnc1fGvrr3KJgBZJsT5H0nQkxAb47Tn9ueeCE1ixJYefPPQhK7Yc9DssY0w9hbPHMwzYoKqbVLUEmAOMq7LPOOAZ93w+cIqIiCufo6rFqroZ2ODqq7FOVX1NHWAp0DmMbTtquUVlgPV4jsT5Qzrz0rWjCMQIFz2xmBeXbvE7JGNMPYQz8XQCtga93ubKqt1HVcuAQ0BGLcfWWae7xDYJeCOoeKSIrBSR10WkX3XBisjVIpItItl794b/uyOH7FLbUenfKZ2XbxjNiF4Z3PG3z7ht/iqKy8r9DssYE4JoHFzwKPBvVV3oXn8CdFPVE4CHgH9Ud5CqPqmqWaqalZmZGfYgK+/xtEi0xHOkWqXE89QVQ7nhB72Zm72VS6bbfR9jmoJwJp7tQJeg151dWbX7iEgskA7sr+XYWusUkf8BMoGbK8tUNVdV893z14A4EWlzNA1rCNbjaRiBGOGW047lkYsHs3rHIcY9/BFf7LJlFoxpzMKZeJYBfUSkh4jE4w0WWFBlnwXA5e75eOA9d49mATDBjXrrAfTBu29TY50iMgU4DZioqt+MtRWR9u6+ESIyDK/N+8PS4nrILaocXGCJpyGcObAD86aNpKyigvMfXcQ7a3f7HZIxpgZhSzzuns0NwJvA58A8VV0jIneJyNlut5lAhohswOul3O6OXQPMA9bi3au5XlXLa6rT1fU40A74uMqw6fHAahFZCTwITNBGMPXxocJS4mNjmvXM1A1tYOeW/PP60fRqm8rU57J54oONNsu1MY2Q2H/M78rKytLs7OywnuOOv33G22t3k/2rU8N6nuaosKScW+av5NVVOxk/pDO/P7c/CbGW4I0JNxFZrqpZde1nXyLxSW5hKen2HZ6wSIoP8PDEQfRpm8r976xn28HDPDEpy+6nGdNIROOotiYht6jU7u+EkYhw06nHcP9FJ7L864OMf2wR23MK/Q7LGIMlHt8cKiy1v8Aj4JxBnXjmqmHsyi3i3Ec+Ys2OQ36HZEyzZ4nHJ7mFpfYdnggZ1asN868ZRWyMcOHjH9vicsb4zBKPT6zHE1nHtk/jb9d9jy6tk7nq6WXMW7a17oOMMWFhiccHqkpuUZlNEBph7dMT+es1IxnVK4NbX1rFfW9/acOtjfGBJR4fFJSUU16h1uPxQVpiHLOuGMr4IZ154N31/PqfqymvsORjTCTZn9w+sHna/BUXiOHP4weSkRrPEx9sIudwKfdeeCLxsfZ3mDGRYInHB/9Zi8cSj19EhDtOP55WyfH88fUvOFRYyhOThpAcb/8ljAk3+xPPB/nF3lo8aYn2Iee3a07uxZ/OH8BHG/ZxyYwl5Bwu8TskY6KeJR4f5LtF4FITLPE0BhcN7cqjlwxhzfZcLnziY3YdKvI7JGOimiUeH1iPp/EZ2789T181lB05RZz/2CI27yvwOyRjopYlHh9UJp4U6/E0KqN6teHFqSMoLC3ngscX2bo+xoSJJR4fFBTbpbbGakDndOZNG0lsTAwTnlzMZ9tsih1jGpolHh/kuXs8KTaCqlHq3TaVedNGkpoQy8XTF7P86wN+h2RMVLHE44P84jJS4gPExIjfoZgadM1IZt60kWSkxjNp5lI+3uj7orXGRA1LPD4oKC4j1QYWNHodWyYxb9pIOrVM4oqnltrkosY0EEs8PsgrLrOBBU1E2xaJzLl6BL0yU5n6TDZvrdnld0jGNHmWeHyQX1RGmiWeJiMjNYEXp47g+I4tuO75T3h55Q6/QzKmSbPE4wO71Nb0pCfHMXvyMAZ3bcXP5qzgpeXb/A7JmCbLEo8PvMEFlniamrTEOJ65ahgje2Vwy/yVlnyMOUKWeHyQV2Q9nqYqKT7AjMuG8r1ebSz5GHOELPH4oKDE7vE0ZUnxAaZflmXJx5gjZIknwlSV/CIb1dbUWfIx5siFNfGIyFgRWSciG0Tk9mq2J4jIXLd9iYh0D9p2hytfJyKn1VWniDzvyleLyCwRiXPlIiIPuv1XicjgcLa5LsVlFZRVqF1qiwKWfIw5MmFLPCISAB4BTgf6AhNFpG+V3SYDB1W1N3Af8Cd3bF9gAtAPGAs8KiKBOup8HjgOGAAkAVNc+elAH/e4Gnis4Vsbum9mprYeT1Sw5GNM/YWzxzMM2KCqm1S1BJgDjKuyzzjgGfd8PnCKiIgrn6Oqxaq6Gdjg6quxTlV9TR1gKdA56BzPuk2LgZYi0iFcja5L5Vo8dqkteljyMaZ+wpl4OgFbg15vc2XV7qOqZcAhIKOWY+us011imwS8UY84EJGrRSRbRLL37g3f1Cj5NjN1VKqafOZb8jGmRnUmHhE5RkTeFZHV7vVAEflV+EM7Yo8C/1bVhfU5SFWfVNUsVc3KzMwMU2iWeKJZUnyAGZd7yecX81fyz0+3+x2SMY1SKD2e6cAdQCmAqq7Cu/9Sl+1Al6DXnV1ZtfuISCyQDuyv5dha6xSR/wEygZvrGUfEfLPstQ0uiEqJcV7PZ1j31tw8byWvf7bT75CMaXRCSTzJqrq0SllZCMctA/qISA8RicdLVguq7LMAuNw9Hw+85+7RLAAmuFFvPfAGBiytrU4RmQKcBkxU1Yoq57jMjW4bARxSVd8+DQpKrMcT7ZLiA8y6YigndmnJjXNW8O7nu/0OyZhGJZTEs09EegEKICLjgTo/uN09mxuAN4HPgXmqukZE7hKRs91uM4EMEdmA10u53R27BpgHrMW7V3O9qpbXVKer63GgHfCxiHwqIr9x5a8Bm/AGKEwHrguhzWFTuQicJZ7olpIQy1NXDuX4Di24dvYn/NuWVDDmG+J1MGrZQaQn8CQwCjgIbAYuUdWvwx+eP7KysjQ7OzssdT/xwUb+8PoXrL3rNJJtvraol3O4hInTl7B5Xz5PXeHN82ZMtBKR5aqaVdd+ofR4VFVPxbt3cpyqjg7xOFON/OIyYgSS4gJ+h2IioGVyPLMnD6NLq2QmP7PMltE2htASyEsAqlqgqnmubH74QopueW66HO/rSqY5yEhN4Pkpw2nXIpErZi1j1bYcv0Myxlc1Jh4ROU5EzgfSReS8oMcVQGLEIowyBcU2QWhz1LZFIi9MHU7LlDgmzVzK2h25fodkjG9q6/EcC5wFtAR+EvQYDEwNe2RRKt+WvW62OqQn8cKUESTHB7h05hLW786r+yBjolCNiUdV/6mqVwJnqeqVQY8bVXVRBGOMKvm2+miz1qV1Mi9MHUEgRrh4xhI27c33OyRjIi6UezwrROR6EXnUzfo8S0RmhT2yKJVfXGZDqZu5Hm1SeGHKcCoqlEtnLGF7TqHfIRkTUaEknueA9nhfzvwA75v/do3gCOUXWeIx0KddGs9OHkZecRmXzljC3rxiv0MyJmJCSTy9VfXXQIGqPgOcCQwPb1jRq8B6PMbp1zGdp68cyq5DRUyauYScwyV+h2RMRISSeErdvzki0h9vPrW24QspuuXZPR4TZEi31ky/LItNewu44qll30wia0w0CyXxPCkirYBf4c17tha3YJupH1W1Ho/5jtF92vDQxYP4bPshpj6TTVFpud8hGRNWdSYeVZ2hqgdV9d+q2lNV2wKvRyC2qFNYWk6F2jxt5rtO69eev1wwkI837eeGFz6htLyi7oOMaaJqTTwiMlJExotIW/d6oIi8AHwUkeiijC2JYGpz7qDO/Pac/rzz+R5+Pm8l5RW1z6NoTFNV4yegiPwZ7wuknwK3icibwBTgD8BVEYkuyuTZInCmDpNGdCO/qIw/vfEFKQkB/u/cATa9kok6tX0CngkMUtUid49nK9BfVb+KSGRRqMASjwnBtWN6kV9cyiPvbyQ1IZb/PuN4Sz4mqtT2CVikqkUAqnpQRNZb0jk6lZfabMocU5dbfnws+UVlTF+4mbTEOG48pY/fIRnTYGr7BOwpIsErhvYIfq2qZ1dzjKmFXWozoRIR/ucn/cgrLuPet78kNSGWq0b38DssYxpEbZ+A46q8viecgTQHlZfa0mxwgQlBTIxw9/kDKSgu465X1pKaGMuFWV38DsuYo1bjJ6CqfhDJQJqDyi8H2qU2E6rYQAwPThzElGeyuf2lVaTEx3LmwA5+h2XMUbGVRCMo3y61mSOQEBvgiUlDGNy1FTfNXcH76/b4HZIxR8USTwTlF5URFxASYu3HbuonOT6WWVcO5Zh2aVw7ezlLN9sS2qbpsk/ACKpcBM6Gxpoj0SIxjmeuGkbHlklMfnoZq7cf8jskY45InYlHRF4WkQVVHs+JyM9ExJbArgdbi8ccrTapCTw/ZTgtkuK4bNZSNuyxFUpM0xNKj2cTkA9Md49cvPV4jnGvTYhsLR7TEDqkJzF7ynBiRLh0xlK2Hjjsd0jG1EsoiWeUql6sqi+7x6XAUFW9Hhgc5viiivV4TEPp0SaF2VOGUVhazqUzl7Ant8jvkIwJWSiJJ1VEula+cM9T3ctaV64SkbEisk5ENojI7dVsTxCRuW77EhHpHrTtDle+TkROq6tOEbnBlamItAkqHyMih0TkU/f4TQhtDosCW4vHNKDj2rfgqSuHsjevmEkzl9pCcqbJCCXx/Bz4UETeF5F/AQuBW0QkBXimpoNEJAA8ApwO9AUmikjfKrtNBg6qam/gPtw6P26/CUA/YCzwqIgE6qjzI+BU4Otqwlmoqie6x10htDks8tzgAmMayuCurZhxWRab9xdwuS0kZ5qIUNbjeQ3oA9wE/Aw4VlVfVdUCVb2/lkOHARtUdZOqlgBz+O5sCOP4T/KaD5wi3pCvccAcVS1W1c3ABldfjXWq6orGPpdcQXEZaZZ4TAMb1bsND08cxGpbSM40EaEOpx6C1/s4AbhQRC4L4ZhOeDNaV9rmyqrdR1XLgENARi3HhlJndUaKyEoReV1E+lW3g4hcLSLZIpK9d+/eEKqsv/wi6/GY8Phxv/bcc8EJLN5sC8mZxi+U4dTPAX8BRgND3SMrzHE1pE+Abqp6AvAQ8I/qdlLVJ1U1S1WzMjMzGzyIigqloKTcBheYsDlnUCfuOrsf73y+h1v+upIKW0jONFKhfApmAX1Vtb6/xduB4BkNO7uy6vbZJiKxQDqwv45j66rzW1Q1N+j5ayLyqIi0UdV99WjLUSsosQlCTfhNGtmdvOIy7n5jHakJsfzunP72hWXT6IRyqW010P4I6l4G9BGRHiISjzdYYEGVfRYAl7vn44H3XIJbAExwo9564N1jWhpind8iIu3dfSNEZBhem/cfQXuOik0QaiLlujG9uebkXjy/ZAt3v7nO73CM+Y5QPgXbAGtFZClQXFlY13o8qlomIjcAbwIBYJaqrhGRu4BsVV0AzASeE5ENwAG8RILbbx6wFigDrlfVcvCGTVet05XfCNyKlyRXichrqjoFL6FdKyJlQCEw4Qh6b0fNVh81kXTb2GPJKyrlsX9tJC0xluvG9PY7JGO+Ecqn4J1HWrkbEfdalbLfBD0vAi6o4djfA78PpU5X/iDwYDXlDwMP1zf2hpZXZInHRI6I8Ntx/cl3l93SEuOYNKKb32EZA4SQeGxdnoZRUOwNcbUvkJpIiYkR/nLBCRQUl/Gbf64mNSHAuYM6+x2WMTXf4xGRD92/eSKSG/TIE5Hcmo4z1csvLgUgJd4Sj4mcuEAMD188mBE9Mrjlr6t4e+1uv0MypubEo6qj3b9pqtoi6JGmqi0iF2J0yHc9HhvVZiItMS7A9Muz6N8pnetf+ISPNkR0QKcx3xHSF0jddDUdRaRr5SPcgUWb/CLX47F7PMYHqQmxPHPlUHpkpDD12WxWbDnod0imGQvlC6Q/BXYDbwOvuscrYY4r6hSUeD2elISAz5GY5qplcjzPTR5GZloCVzy1jM932hVz449QejyV87P1U9UB7jEw3IFFm7yiMuIDMSTEWuIx/mnbIpHZk4eTFBdg0sylfLWvwO+QTDMUSuLZijeHmjkKtiSCaSy6tE5m9pRhVKhyyYwl7Mgp9Dsk08yEugLpv9z6ODdXPsIdWLTJLy6zy2ym0ejdNo1nrxpGbmEpl85cwv784roPMqaBhJJ4tuDd34kH0oIeph7yispITYjzOwxjvtG/UzozrxjKjpxCLpu1lEOFpX6HZJqJWq/9uIXXjlHVSyIUT9QqKC4j1Xo8ppEZ1qM1j186hKnPZjP56WU8N3k4SfH2e2rCq9Yej5sfrZubkNMchfziMpsuxzRKY45ty/0XDeKTLQeZNns5xWW2kJwJr1A+CTcBH4nIAuCbITCqem/YoopCBcVldMtI9jsMY6p15sAOFBQP5NaXVnHTnE95aOIgYgOhrhNpTP2E8pu1Ee97OzHYPZ4jlldcZrMWmEbtwqFd+NWZx/P66l3c8bfPbCE5EzahTBL6v5EIJNoVFJfZPG2m0Zvy/Z7kFZXxwLvrSUuM49dnHW8LyZkGV+cnoYhk4q1z0w9IrCxX1R+GMa6oUl6hHC4pt+/xmCbhplP7kFtUyqyPNtMiKZabTj3G75BMlAnlk/B5YC5wFnAN3oqhe8MZVLSpXPbaBheYpkBE+PWZfckrKuP+d7yez+TRPfwOy0SRUD4JM1R1poj8zK3N84GILAt3YNEk3xaBM01MTIzwx/MGUFBcxm9fWUtaQiwXDu3id1gmSoQyuKDyW2U7ReRMERkEtA5jTFGnctlrm5naNCWxgRjun3Ai3+/Thtv/topXV+30OyQTJUJJPL8TkXTg58AtwAzgv8IaVZTJc4nH7vGYpiYhNsATk4YwuGsrbpq7gvfX7fE7JBMF6kw8qvqKqh5S1dWq+gNVHaKqCyIRXLSo7PHYpTbTFCXHxzLziqEc2z6Nac8t58P1tpCcOTqhrMdzjIi8KyKr3euBIvKr8IcWPewej2nq0pPieO6q4fRsk8KUZ5fx8cb9fodkmrBQLrVNB+7A3etR1VXAhHAGFW3yrMdjokCrlHienzKcLq2SuerpZSzdfMDvkEwTFUriSVbVpVXKysIRTLSyS20mWmSkJvD81OF0aJnIlU8tZfnXtoS2qb9QEs8+EekFKICIjAdseEs9VF5qs1FtJhq0TUvkxakjvCW0Zy1l5dYcv0MyTUwoied64AngOBHZDtyE90XSOonIWBFZJyIbROT2arYniMhct32JiHQP2naHK18nIqfVVaeI3ODKVETaBJWLiDzotq0SkcGhxN6Q8kvKiI+NIT7WJl000aFdi0RemDqClilxTJq5hNXbbZFiE7pQRrVtUtVTgUzgOFUdDZxb13FuLZ9HgNOBvsBEEelbZbfJwEFV7Q3cB/zJHdsX7z5SP2As8KiIBOqo8yPgVODrKuc4HejjHlcDj9UVe0PLL7IlEUz06dgyiRemjCAtMY5LZy7h8525fodkmoiQ/wRX1QJVzXMvQ1n6ehiwwSWuEmAOMK7KPuOAZ9zz+cAp4s1IOA6Yo6rFqroZ2ODqq7FOVV2hql9VE8c44Fn1LAZaikiHEJvdIApsLR4Tpbq0TubFqSNIjA1wyYwlfLk7r+6DTLN3pNd+QpmuthOwNej1NldW7T6qWgYcAjJqOTaUOo8kDkTkahHJFpHsvXsbdiq6/OIyu79jolbXjGRevHoEsTHCxdOXsHFvvt8hmUbuSBNP1C3UoapPqmqWqmZlZmY2aN35xWWkWeIxUaxHmxRemDoCUCY+uZjN+wrqPMY0XzUmHhHJE5Hcah55QMcQ6t4OBM8q2NmVVbuPiMQC6cD+Wo4Npc4jiSOs8orKbLocE/V6t03lhakjKK9QLnriY+v5mBrVmHhUNU1VW1TzSFPVUD5FlwF9RKSHiMTjDRaoOtXOArxlFgDGA++pqrryCW7UWw+8gQFLQ6yzqgXAZW502wjgkKpGdDh4XpGtPmqah2PapfHi1SOoUOWiJxaz3u75mGqEbXyvu2dzA/Am8DkwT1XXiMhdInK2220mkCEiG/AGLNzujl0DzAPWAm8A16tqeU11AojIjSKyDa9Hs0pEZrhzvAZswhugMB24LlxtrkluUSktEuMifVpjfHFMuzTmXD0CEZg4fTHrdlnyMd8mXgfDBMvKytLs7OwGqUtV6f3L15l2Uk9uHXtcg9RpTFOwcW8+E59cTFmF8vyU4RzfoYXfIZkwE5HlqppV1372jcYwKywtp7xCSbMej2lmemWmMnfaSOIDMVw8fTFrdtiXTI3HEk+Y5RZ60+W0SLJ7PKb56dEmhbnTRpAUF+Di6TbDgfFY4gmzvCJvAVfr8ZjmqltGCnOnjSQ1IZaLpy+2ud2MJZ5wy3UThLawUW2mGevSOpk5V48gPdmbXmfFFpvVujmzxBNmudbjMQbwks/cq0fSOiWeS2csscXkmjFLPGGWZz0eY77RsWUS86aNpGPLJK54ainvf7HH75CMDyzxhFnlPZ4WSdbjMQa8JRXmThvJMe3SmPpsNq+s2uF3SCbCLPGEWeWoNpu5wJj/aJ0Sz/NThzOoa0tufHEF85ZtrfsgEzUs8YRZXlEpgRghKS7gdyjGNCotEuN49qrhjO6Tya0vrWLmh5v9DslEiCWeMMsrKqNFYizeMkPGmGBJ8QGmXzaE0/u357evrOXBd9djs6lEP0s8YZZbVGoj2oypRUJsgIcmDuL8wZ259+0v+b/XPrfkE+XsxkOY2czUxtQtNhDDn8cPJDUhwPSFmzl4uJQ/nDeAuID9bRyN7BMxzPJsZmpjQhITI9x5dj9aJsfzwLvrOVBQwiMXDyYp3u6PRhv7cyLMcgutx2NMqESE//rRMfzunP78a90eLp6xmIMFJX6HZRqYJZ4wyysqte/wGFNPl47oxqOXDGHNjlzOf3wR2w4e9jsk04As8YSZ3eMx5siM7d+e2ZOHsy+vmPMfW8QXu3L9Dsk0EEs8YVReoeQVl9moNmOO0LAerfnrNaMQhAse/5glm2x+t2hgiSeMcgu96XJaJVviMeZIHds+jZeuG0XbtAQmzVpqU+xEAUs8YZTjEk9LSzzGHJVOLZOYf80oTuiczg0vrOCR9zfYd32aMEs8YXTwsDcap2VSvM+RGNP0tUqJZ/aU4Yw7sSN/fnMdt720ipKyCr/DMkfA7nqH0aHD1uMxpiElxAa4/6IT6Z6RwgPvrmfbwUIeu2QI6fZ/rEmxHk8Y5RS6Hk+y9XiMaSiV3/W554ITWPbVAc577CO27Lfh1k2JJZ4wOljgejz2PR5jGtz5Qzrz3OTh7Msv4dxHP2L51wf8DsmEyBJPGOUUliJii8AZEy4jembw9+tGkZYYy8QnlzAv29b1aQrCmnhEZKyIrBORDSJyezXbE0Rkrtu+RES6B227w5WvE5HT6qpTRHq4Oja4OuNd+RUisldEPnWPKeFsc7CcwyW0SIwjEGNLIhgTLj0zU/nH9d9jWI/W3Dp/FXcuWENpuQ06aMzClnhEJAA8ApwO9AUmikjfKrtNBg6qam/gPuBP7ti+wASgHzAWeFREAnXU+SfgPlfXQVd3pbmqeqJ7zAhDc6uVc7jUvsNjTAS0TI7n6SuHMnl0D55e9BWXzVzKAZvjrdEKZ49nGLBBVTepagkwBxhXZZ9xwDPu+XzgFPFWTBsHzFHVYlXdDGxw9VVbpzvmh64OXJ3nhK9pockpLCXdBhYYExGxgRh+fVZf7rngBJZvOcjZD3/I2h02zU5jFM7E0wkIvuC6zZVVu4+qlgGHgIxajq2pPAPIcXVUd67zRWSViMwXkS7VBSsiV4tItohk7927N/RW1iLncIkNLDAmws4f0pm/ThtJWbly/mOLbKaDRqg5DC54GeiuqgOBt/lPD+tbVPVJVc1S1azMzMwGObFdajPGHyd0acmCn36Pvh1bcMMLK7jr5bX2ZdNGJJyJZzsQ3Lvo7Mqq3UdEYoF0YH8tx9ZUvh9o6er41rlUdb+qFrvyGcCQo2pVPeQcLrHv8Bjjk7Zpibw4dQRXjOrOrI82c9GTH7Mjp9DvsAzhTTzLgD5utFk83mCBBVX2WQBc7p6PB95TbwKmBcAEN+qtB9AHWFpTne6Y910duDr/CSAiHYLOdzbweQO3s1ql5RXkFpXZrAXG+Cg+NoY7z+7HIxcPZv3ufM58cCH/WrfH77CavbAlHne/5QbgTbwP+3mqukZE7hKRs91uM4EMEdkA3Azc7o5dA8wD1gJvANeranlNdbq6bgNudnVluLoBbhSRNSKyErgRuCJcbQ62P98bUdMmNSESpzPG1OLMgR1YcMP3aNcikSufXsY9b62jvMImGfWL2Ayv35WVlaXZ2dlHVcfq7Yc466EPefzSIYzt376BIjPGHI3CknJ+88/V/HX5Nkb0bM19F51Ih/Qkv8OKGiKyXFWz6tqvOQwu8MW+fO+2Umaa3eMxprFIig/w5wtO4O7xA1m59RBj71/IG6t3+h1Ws2OJJ0z22aU2YxqtC7O68OqNo+mWkcw1sz/htvmrKCguq/tA0yAs8YRJZY/HEo8xjVPPzFTmXzOK68b0Yt7yrZz10Ies2pbjd1jNgiWeMNmXV0xSXICUBFvyyJjGKj42hlvHHseLU0dQVFrOeY8u4qF319tcb2FmiSdM9uUXk5Fq93eMaQpG9MzgjZ+dxOkDOnDP219yziMf8flOm24nXCzxhMm+/BK7zGZME5KeHMdDEwfx+KWD2Z1bxNkPf8gD71jvJxws8YTJnrwi2qZZ4jGmqRnbvwNv/9fJnDGgA/e98yXjHv6INTsO+R1WVLHEEyY7coro2NK+H2BMU9QqJZ4HJgziiUlD2JNXzNkPf8TvX11rI98aiCWeMMgtKiW/uIyOLRP9DsUYcxRO69eed24+iQuzOjN94WZOvfcD3li9E/vi/dGxxBMGlRMRWo/HmKavZXI8fzhvIC9dO5L0pDiumf0JVz29jC37D/sdWpNliScMLPEYE32GdGvNKz8dza/OPJ6lmw/wo/s+4J631tnltyNgiScMtucUAdDR5oAyJqrEBmKY8v2evPPzkzmtX3seem8DY/7yL+Yu22KTjtaDJZ4w+HpfAQmxMWTaqDZjolKH9CQenDiIv103ii6tkrjtpc8466EPWbRhn9+hNQmWeMJg4958emamEogRv0MxxoTR4K6teOnaUTx88SDyikq5eMYSJs1cwootB/0OrVGzxBMGG/bm0yszxe8wjDERICKcNbAj79x8Mr8843jW7Mjl3EcXcdXTy1i93b7/Ux1LPA2soLiMbQcL6d021e9QjDERlBgXYOpJPVl46w/4xWnHsvzrg5z10IdMey6bz7ZZAgpmM1g2sJVbc1CFE7u09DsUY4wPUhJiuf4HvZk0shuzPtzMzIWbeXPNbr7XO4NpJ/Xi+33aINK8L8Nbj6eBfbRxHzECg7q28jsUY4yPWiTGcdOpx/DRHT/kjtOPY8OefC6btZQzHvyQf6zYTklZ850DzhJPA1q0cR/Pffw13++TSXpSnN/hGGMagRaJcUw7uRcLb/0hfx4/kNLyCm6a+ymj/vgud7/xBVsPNL8votqltgaUnhRH14xkfvOTvn6HYoxpZOJjY7ggqwvnD+7Mv9fv5fklW3j8g4089sFGfnBsWyYO68qYYzOJC0R/f0BszqHvysrK0uzs7CM6VlWb/fVbY0xoduQUMmfpFl5ctpW9ecW0So7jrIEdOWdQJwZ3bdnkPktEZLmqZtW5nyWe7zqaxGOMMfVVWl7BwvV7+fuKHby1ZhfFZRV0bZ3MGQM68KO+7RjUpSUxTeB7gZZ4joIlHmOMX/KKSnlzzW7+sWI7H2/aT3mF0iY1nlOOa8epfdsxomdr0hIb5z1kSzxHwRKPMaYxOHS4lH99uYe31u7mg3V7yS8uIxAjDOiUzsheGYzqlcHgrq1ISWgct+sbReIRkbHAA0AAmKGqf6yyPQF4FhgC7AcuUtWv3LY7gMlAOXCjqr5ZW50i0gOYA2QAy4FJqlpS2zlqYonHGNPYlJRVkP3VAT7etJ9FG/ezcmsOZRVKjEDvtqkM6NSSgZ3T6d8pnd5tU30ZWet74hGRAPAl8CNgG7AMmKiqa4P2uQ4YqKrXiMgE4FxVvUhE+gIvAsOAjsA7wDHusGrrFJF5wN9UdY6IPA6sVNXHajpHbbFb4jHGNHYFxWUs++oAn27NYdW2Q6zalsO+/JJvtrdJTaBnZgq9MlPp3CqJdi0SadcigXYtEmmTmkBKQoCE2ECDxhRq4gln/2wYsEFVN7mA5gDjgLVB+4wD7nTP5wMPizeMYxwwR1WLgc0issHVR3V1isjnwA+Bi90+z7h6H6vpHGrXGI0xTVhKQixjjm3LmGPbAt6I2l25Razensumvfls3JvPpr0FvLF6JwcPl1ZbR1xASEmIJSU+lriAEIgRJg7rypTv9wxr7OFMPJ2ArUGvtwHDa9pHVctE5BDepbJOwOIqx3Zyz6urMwPIUdWyavav6Rzfmr9cRK4Grgbo2rVrfdppjDG+ExE6pCfRIT0JaPetbYUl5ezJK2J3bjG7c4vYl19MQXEZ+cXlFBSXUVBSRlm5Uq5Km9TwL+fSOO5INQKq+iTwJHiX2nwOxxhjGkxSfIBuGSl0y2gcs+aH8yuy24EuQa87u7Jq9xGRWCAdbwBATcfWVL4faOnqqHqums5hjDHGB+FMPMuAPiLSQ0TigQnAgir7LAAud8/HA++5ey8LgAkikuBGq/UBltZUpzvmfVcHrs5/1nEOY4wxPgjbpTZ3P+UG4E28oc+zVHWNiNwFZKvqAmAm8JwbPHAAL5Hg9puHNxChDLheVcsBqqvTnfI2YI6I/A5Y4eqmpnMYY4zxh32BtBo2nNoYY+ov1OHU0T8NqjHGmEbFEo8xxpiIssRjjDEmoizxGGOMiSgbXFANEdkLfH2Eh7ehyqwIzYC1uXmwNjcPR9PmbqqaWddOlngamIhkhzKqI5pYm5sHa3PzEIk226U2Y4wxEWWJxxhjTERZ4ml4T/odgA+szc2Dtbl5CHub7R6PMcaYiLIejzHGmIiyxGOMMSaiLPE0IBEZKyLrRGSDiNzudzz1JSJfichnIvKpiGS7stYi8raIrHf/tnLlIiIPurauEpHBQfVc7vZfLyKXB5UPcfVvcMeKD22cJSJ7RGR1UFnY21jTOXxs850ist2915+KyBlB2+5w8a8TkdOCyqv9/XbLlCxx5XPdkiW4ZU3muvIlItI9Qk1GRLqIyPsislZE1ojIz1x51L7XtbS58b3XqmqPBnjgLdOwEegJxAMrgb5+x1XPNnwFtKlSdjdwu3t+O/An9/wM4HVAgBHAElfeGtjk/m3lnrdy25a6fcUde7oPbTwJGAysjmQbazqHj22+E7ilmn37ut/dBKCH+50O1Pb7DcwDJrjnjwPXuufXAY+75xOAuRFscwdgsHueBnzp2ha173UtbW5073VE/9NH8wMYCbwZ9PoO4A6/46pnG77iu4lnHdDBPe8ArHPPnwAmVt0PmAg8EVT+hCvrAHwRVP6t/SLczu58+0M47G2s6Rw+trmmD6Nv/d7irX01sqbfb/ehuw+IdeXf7Fd5rHse6/YTn97zfwI/ag7vdTVtbnTvtV1qazidgK1Br7e5sqZEgbdEZLmIXO3K2qnqTvd8F9DOPa+pvbWVb6umvDGIRBtrOoefbnCXlWYFXQ6qb5szgBxVLatS/q263PZDbv+Icpd9BgFLaCbvdZU2QyN7ry3xmGCjVXUwcDpwvYicFLxRvT9nonr8fSTa2Eh+jo8BvYATgZ3APb5GEyYikgq8BNykqrnB26L1va6mzY3uvbbE03C2A12CXnd2ZU2Gqm53/+4B/g4MA3aLSAcA9+8et3tN7a2tvHM15Y1BJNpY0zl8oaq7VbVcVSuA6XjvNdS/zfuBliISW6X8W3W57elu/4gQkTi8D+DnVfVvrjiq3+vq2twY32tLPA1nGdDHjfqIx7vBtsDnmEImIikiklb5HPgxsBqvDZUjeS7Hu26MK7/MjQYaARxylxfeBH4sIq1cl/7HeNeBdwK5IjLCjf65LKguv0WijTWdwxeVH4zOuXjvNXhxTnCjlHoAffBuolf7++3+on8fGO+Or/rzq2zzeOA9t3/YuZ//TOBzVb03aFPUvtc1tblRvtd+3PSK1gfeyJgv8UaE/NLveOoZe0+80SsrgTWV8eNdp30XWA+8A7R25QI84tr6GZAVVNdVwAb3uDKoPMv90m8EHsaHG83Ai3iXG0rxrlFPjkQbazqHj21+zrVplfvQ6BC0/y9d/OsIGnlY0++3+91Z6n4WfwUSXHmie73Bbe8ZwTaPxrvEtQr41D3OiOb3upY2N7r32qbMMcYYE1F2qc0YY0xEWeIxxhgTUZZ4jDHGRJQlHmOMMRFliccYY0xEWeIxJoJEpDxoluBPpQFnMReR7hI0A7UxjVVs3bsYYxpQoaqe6HcQxvjJejzGNALirYV0t3jruywVkd6uvLuIvOcmeHxXRLq68nYi8ncRWekeo1xVARGZLt56LG+JSJLb/0bx1mlZJSJzfGqmMYAlHmMiLanKpbaLgrYdUtUBeN+Cv9+VPQQ8o6oDgeeBB135g8AHqnoC3lo7a1x5H+ARVe0H5ADnu/LbgUGunmvC0zRjQmMzFxgTQSKSr6qp1ZR/BfxQVTe5iR53qWqGiOzDm+Kk1JXvVNU2IrIX6KyqxUF1dAfeVtU+7vVtQJyq/k5E3gDygX8A/1DV/DA31ZgaWY/HmMZDa3heH8VBz8v5z33cM/HmIhsMLAuaYdiYiLPEY0zjcVHQvx+754vwZgcGuARY6J6/C1wLICIBEUmvqVIRiQG6qOr7wG14U9Z/p9dlTKTYXz3GRFaSiHwa9PoNVa0cUt1KRFbh9VomurKfAk+JyC+AvcCVrvxnwJMiMhmvZ3Mt3gzU1QkAs11yEuBBVc1poPYYU292j8eYRsDd48lS1X1+x2JMuNmlNmOMMRFlPR5jjDERZT0eY4wxEWWJxxhjTERZ4jHGGBNRlniMMcZElCUeY4wxEfX/w+duaJIYCW4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lrs  = []\n",
    "for  i  in  range(250000):\n",
    "\tscheduler.step()\n",
    "\tlrs.append(optim.param_groups[0][\"lr\"])  \n",
    "\n",
    "plt.plot(lrs)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"OneCycleLR Scheduler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGqeHeQRxxLY"
   },
   "source": [
    "### ConformerSASwiGLULayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "saxlznhLxxLY"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ConformerSASwiGLULayer(nn.Module):\n",
    "    def __init__(self, embedding_dim,  ff_mult = 4, kernel_size = 15, rate = 0.2, num_heads = 4, use_bias = False):\n",
    "        super(ConformerSASwiGLULayer, self).__init__()\n",
    "        self.ff1 = FeedForwardSwiGLU(embedding_dim = embedding_dim, mult = ff_mult, rate = rate, use_bias = use_bias)\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim,eps = 1e-6)\n",
    "        self.conv = nn.Sequential(   \n",
    "          nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=kernel_size, groups=embedding_dim, padding='same'),\n",
    "          nn.Conv1d(in_channels=embedding_dim, out_channels=embedding_dim, kernel_size=1, padding='same'),\n",
    "          nn.ReLU(),\n",
    "          nn.Dropout(rate),\n",
    "        )\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim,eps = 1e-6)    \n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=num_heads,batch_first=True)\n",
    "        self.ff2 = FeedForwardSwiGLU(embedding_dim = embedding_dim, mult = ff_mult, rate = rate, use_bias = use_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = x + 0.5 * self.ff1(x)\n",
    "        x = self.layernorm1(x.transpose(1,2)) #채널 dim = 2\n",
    "        x = x + self.conv(x.transpose(1, 2)).transpose(1, 2) # output 채널 dim = 2\n",
    "        x = self.layernorm2(x)\n",
    "        x = x + self.attn(x, x, x)[0]\n",
    "        x = x.transpose(1,2) + 0.5 * self.ff2(x.transpose(1,2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5mMf8GPxxLZ",
    "outputId": "92e9877e-f3ac-49da-b3f5-325d74b60719"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 16, 10])\n"
     ]
    }
   ],
   "source": [
    "layer = ConformerSASwiGLULayer(embedding_dim = 16)\n",
    "x = torch.randn(3,16,10)\n",
    "print(layer(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKbk2vhOTgA5"
   },
   "source": [
    "### SequenceMaskLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Q7WXIVfClKlE"
   },
   "outputs": [],
   "source": [
    "class SequenceMaskLayer(nn.Module):\n",
    "    def __init__(self, n_positions, ratio = 0.2):\n",
    "        super(SequenceMaskLayer, self).__init__()\n",
    "        self.ratio = ratio\n",
    "        self.n_positions = n_positions\n",
    "        self.N = 4\n",
    "        self.M = 5\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.ratio > 0:\n",
    "            m = torch.rand(x.shape) < self.ratio\n",
    "            m = m*1\n",
    "            is_valid = x == self.N\n",
    "            is_valid = is_valid * 1\n",
    "            m = m * is_valid\n",
    "            x0 = torch.ones(x.shape) * self.M\n",
    "\n",
    "            x = m * x0 + (1 - m) * x\n",
    "            m = m.float()\n",
    "        else:\n",
    "            m = torch.zeros(x.shape)\n",
    "    \n",
    "        return x, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOj2tUQmTgA6",
    "outputId": "8a271106-304b-4b95-83a1-232664591280"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 0., 0., 0., 4., 4., 1., 0., 4., 1., 0., 2., 1., 1., 0., 4., 2., 0.,\n",
      "         0., 3.],\n",
      "        [0., 2., 2., 3., 3., 0., 1., 3., 1., 3., 4., 2., 1., 5., 5., 1., 1., 0.,\n",
      "         2., 3.],\n",
      "        [2., 1., 1., 1., 3., 1., 0., 1., 4., 3., 3., 2., 3., 4., 0., 4., 2., 1.,\n",
      "         0., 2.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "         0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "layer = SequenceMaskLayer(n_positions = 20, ratio = 0.2)\n",
    "x = torch.rand([3,20])*5\n",
    "x = torch.floor(x)\n",
    "x, m = layer(x)\n",
    "print(x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGFLv3peCCKS"
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "om_YoxPICCKS"
   },
   "outputs": [],
   "source": [
    "input_dim = int(6) # A,C,G,T,N,M\n",
    "#input_dim = int(5) # A,C,G,T,N\n",
    "n_positions = ARGS['max_width'] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbU-MCtK1B7k",
    "outputId": "3129bc87-a86a-49c5-b203-fa4af6e201ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downsampled dataset size: 6737568\n",
      "training dataset size: 6400689\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data['seq']))\n",
    "n_train = int(n * ARGS['train_split'])\n",
    "print('downsampled dataset size: %d' % (n))\n",
    "print('training dataset size: %d' % (n_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zQBqfMrpCCKT",
    "outputId": "51b10fec-522a-42f8-d692-4fa9e8eb19b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training samples: 6400689\n",
      "# val samples: 336879\n"
     ]
    }
   ],
   "source": [
    "train_data = {'seq':data['seq'][:n_train],'expression':data['expression'][:n_train]}\n",
    "val_data = {'seq':data['seq'][n_train:],'expression':data['expression'][n_train:]}\n",
    "\n",
    "print('# training samples: %d' % (len(train_data['seq'])))\n",
    "print('# val samples: %d' % (len(val_data['seq'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2xtZGxPzz4s"
   },
   "source": [
    "# DataLoader & TestSet Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "O5LDHmegztZL"
   },
   "outputs": [],
   "source": [
    "class train_loader(object):\n",
    "    def __init__(self, data):\n",
    "        self.data = data['seq']\n",
    "        self.data_label = data['expression']\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.data[index]), self.data_label[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "class test_loader(object):\n",
    "    def __init__(self,args):\n",
    "        lines = open(\"/Data1/PGE/torch_ti/filtered_test_data_with_MAUDE_expression.txt\", \"r\").read().splitlines()\n",
    "        data = [x.split('\\t')[0] for x in lines]\n",
    "        data_label = [x.split('\\t')[1] for x in lines]\n",
    "        df = pd.DataFrame()\n",
    "        df['dna'] = data\n",
    "        df['expression'] = data_label\n",
    "        df['dna'] = df['dna'].astype('string')\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        print('number of unique sequences in the first {} positions: {}'.format(args['head_len'], len(df['dna'].str[:args['head_len']].unique())))\n",
    "        print('number of unique sequences in the last {} positions: {}'.format(args['tail_len'], len(df['dna'].str[-args['tail_len']:].unique())))\n",
    "        df['dna'] = df['dna'].str[args['head_len']:]\n",
    "        df['dna'] = df['dna'].str[:-args['tail_len']]\n",
    "        df['len'] = df['dna'].str.len()\n",
    "        assert all(df['len'] <= args['max_width'])\n",
    "        \n",
    "        df['dna'] = df['dna'].str.pad(width = args['max_width'], side = 'both', fillchar = 'N')\n",
    "        df['dna'] = df['dna'] + df['dna'].apply(lambda x: str(Seq(x).reverse_complement())).astype('string')\n",
    "        \n",
    "        input_dim = int(6) # A,C,G,T,N,M\n",
    "        n_positions = int(args['max_width'] * 2)\n",
    "        self.dna = np.empty((0, n_positions), np.uint8)\n",
    "        for x in np.array_split(df['dna'], 10): # split data into chunks\n",
    "            y = np.array(x.apply(list))\n",
    "            y = np.vstack(y)\n",
    "            y = np.vectorize(ARGS['alphabets'].get)(y)\n",
    "            y = y.astype(np.uint8)\n",
    "            print(y.shape)\n",
    "            self.dna = np.append(self.dna, y, axis = 0)\n",
    "        print(self.dna.shape)\n",
    "        self.expression = df['expression'].astype('float32').to_numpy()\n",
    "        expression_std = np.std(self.expression)\n",
    "        expression_mean = np.mean(self.expression)\n",
    "        self.expression = (self.expression - expression_mean) / expression_std\n",
    "        \n",
    "        print(self.expression.shape)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        return torch.FloatTensor(self.dna[index]), self.expression[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KwOLqNOwU3U"
   },
   "source": [
    "## The regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, strides = 2, ratio = 0.2, ff_mult = 4, use_bias = False, num_projectors = 8):\n",
    "        super(FirstLayer_Block, self).__init__()\n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.input_dim = input_dim\n",
    "        self.kmer = kmer\n",
    "        self.strides = strides\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        self.masking = SequenceMaskLayer(n_positions = n_positions, ratio = ratio)\n",
    "        self.pos_embedding = nn.Embedding(self.n_positions, embedding_dim)\n",
    "        self.strand_embedding = nn.Embedding(2, embedding_dim) # plus/minus strands\n",
    "        self.expression_embedding = nn.Linear(1,embedding_dim)\n",
    "        self.kmer_dense = nn.Linear(input_dim*self.kmer,embedding_dim)\n",
    "       \n",
    "\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq)\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = F.one_hot(x.to(torch.int64), self.input_dim)   # output = (b,seq,embed)\n",
    "\n",
    "        x = x.unsqueeze(2)  # b,seq,em,1\n",
    "        x_shape = x.shape\n",
    "        fold_shape = x.unfold(1,self.kmer,self.strides).transpose(3,4).shape\n",
    "        div = x_shape[1] - fold_shape[1]\n",
    "        x = F.pad(x.unfold(1,self.kmer,self.strides).transpose(3,4),(0,0,0,0,0,0,0,div),'constant',0).reshape(x.shape[0],x.shape[1]//self.strides,x.shape[2],-1)\n",
    "        x = x.squeeze(2).float()\n",
    "        x = self.kmer_dense(x)\n",
    "\n",
    "        pos = torch.arange(start=0, end = self.n_positions, step=1).cuda()\n",
    "        pos = pos.unsqueeze(0)\n",
    "        pos = self.pos_embedding(pos.long())\n",
    "\n",
    "        strand = torch.tensor(np.repeat([0,1], repeats = int(self.n_positions / 2))).cuda()\n",
    "        strand = strand.unsqueeze(0)\n",
    "        strand = self.strand_embedding(strand.long())\n",
    "\n",
    "        x = x + pos + strand  # 채널 dim=2\n",
    "\n",
    "        expression = torch.zeros((batch_size, self.num_projectors, 1)).cuda()\n",
    "        expression = self.expression_embedding(expression.float())\n",
    "\n",
    "        x = torch.cat([expression, x], dim = 1)\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Core_Block(nn.Module):\n",
    "    def __init__(self, embedding_dim = 32, input_dim = 5, n_blocks = 4, \n",
    "               kernel_size =15, rate = 0.2, num_heads = 4):\n",
    "        super(Core_Block, self).__init__()\n",
    "        self.n_blocks = n_blocks\n",
    "        self.blocks = nn.ModuleList([ConformerSASwiGLULayer(embedding_dim = embedding_dim,\n",
    "                                    kernel_size = kernel_size, rate = rate, num_heads = num_heads) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        for i in range(self.n_blocks) :\n",
    "            x = self.blocks[i](x)\n",
    "        \n",
    "        return x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalLayer_Block(nn.Module):\n",
    "    def __init__(self, n_positions, embedding_dim = 32, input_dim = 5, rate = 0.2, strides = 2, use_bias = False, num_projectors = 8):\n",
    "        super(FinalLayer_Block, self).__init__()\n",
    "        \n",
    "        self.n_positions = int(n_positions / strides)\n",
    "        self.num_projectors = num_projectors\n",
    "        \n",
    "        self.dropout = nn.Dropout(rate)\n",
    "        self.expression_dense = nn.Linear(embedding_dim,1)\n",
    "        self.nucleotide_dense = nn.Linear(embedding_dim,input_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "\n",
    "        expression = x[:,:self.num_projectors,:]\n",
    "        x = x[:, -self.n_positions:, :]\n",
    "\n",
    "        expression = self.dropout(expression)\n",
    "        expression = self.expression_dense(expression)\n",
    "        expression = torch.mean(expression, 1)\n",
    "\n",
    "        x = self.nucleotide_dense(x)\n",
    "\n",
    "        return expression, x.transpose(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self, n_positions, kmer = 3, embedding_dim = 32, input_dim = 5, n_blocks = 4, \n",
    "               kernel_size =15, rate = 0.2, strides = 2, ratio = 0.2, num_heads = 4, ff_mult = 4, \n",
    "               use_bias = False, num_projectors = 8):\n",
    "        super(Regressor, self).__init__()\n",
    "        \n",
    "        self.masking = SequenceMaskLayer(n_positions = n_positions, ratio = ratio)\n",
    "        \n",
    "        self.first_block = FirstLayer_Block(n_positions, kmer, embedding_dim, input_dim, strides, \n",
    "                                            ratio, ff_mult, use_bias, num_projectors)\n",
    "        self.core_block = Core_Block(embedding_dim, input_dim, n_blocks, \n",
    "                                     kernel_size, rate, num_heads)\n",
    "        self.final_block = FinalLayer_Block(n_positions, embedding_dim, input_dim, rate,\n",
    "                                       strides, use_bias, num_projectors)\n",
    "\n",
    "    def forward(self, x): # input = (batch, seq)\n",
    "\n",
    "        first_out = self.first_block(x)\n",
    "        core_out = self.core_block(first_out)\n",
    "        expression, seq_out = self.final_block(core_out)\n",
    "\n",
    "        return expression, seq_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "z-jAAiPA0oMS"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class RegressorModel(nn.Module):\n",
    "\tdef __init__(self, args,**kwargs):\n",
    "\t\tsuper(RegressorModel, self).__init__()\n",
    "\t\t## regressor\n",
    "\t\tself.arg = args\n",
    "\t\tself.regressor = Regressor(n_positions = n_positions,embedding_dim = args['embedding_dim'],\n",
    "                             n_blocks = args['n_blocks_regressor'],kmer = args['kmer'],input_dim = input_dim,\n",
    "                             strides = args['strides'],ratio = args['mask_ratio'],num_heads = args['num_heads'],\n",
    "                             rate = args['dropout_rate'],num_projectors = args['num_projectors']).cuda()\n",
    "\t\tself.mse_loss = nn.MSELoss(reduction='none').cuda()\n",
    "\t\tself.scc_loss = nn.CrossEntropyLoss( reduction='none').cuda()\n",
    "\t\tself.optim           = torch.optim.Adam(self.regressor.parameters(), lr = args['initial_lr'], betas=(0.9, 0.98), eps=1e-08)\n",
    "\t\tself.scheduler       = torch.optim.lr_scheduler.OneCycleLR(self.optim, max_lr=args['max_lr'],pct_start = 0.05, \n",
    "                                                                   steps_per_epoch=int(n_train/args['batch_size'])+1, epochs=args['epochs'],anneal_strategy='cos')\n",
    "\t\tprint(time.strftime(\"%m-%d %H:%M:%S\") + \" Model para number(백만) = %.2f\"%(sum(param.numel() for param in self.regressor.parameters()) / 1024 / 1024))\n",
    "\n",
    "\tdef train_network(self, epoch, loader):\n",
    "\t\tself.train()\n",
    "\t\t## Update the learning rate based on the current epoch\n",
    "\t\tif epoch > 0 :\n",
    "\t\t\tself.scheduler.step((epoch - 1)*int(n_train/self.arg['batch_size']))\n",
    "\t\t\tprint('LR : ',self.scheduler.get_last_lr()[0])\n",
    "\t\tindex, loss = 0, 0\n",
    "\t\tfor num, (data, labels) in tqdm(enumerate(loader, start = 1)):\n",
    "\n",
    "\t\t\tself.zero_grad()\n",
    "\t\t\tseq, mask = self.regressor.masking(data)\n",
    "\t\t\tlabels = labels.cuda()\n",
    "\t\t\texpression, seq_pred = self.regressor.forward(data.cuda()) \n",
    "\t\t\tloss_expression = self.mse_loss(labels.to(torch.float32), expression.squeeze(1).to(torch.float32))\n",
    "\t\t\tloss_seq = mask.cuda() * self.scc_loss(seq_pred,data.long().cuda())\n",
    "\t\t\tloss_seq = torch.sum(loss_seq) / (torch.sum(mask.cuda()) + 1)\n",
    "\t\t\tnloss = (loss_expression.to(torch.float32) + loss_seq.to(torch.float32)).mean().to(torch.float32)\n",
    "\t\t\t\n",
    "\t\t\tnloss.backward()\n",
    "\t\t\tself.optim.step()\n",
    "\t\t\tself.scheduler.step()\n",
    "\t\t\tlr = self.scheduler.get_last_lr()[0]\n",
    "\t\t\tindex += len(labels)\n",
    "\t\t\tloss += nloss.detach().cpu().numpy()\n",
    "\t\t\tif num % 200 == 0 :\n",
    "\t\t\t\tsys.stderr.write(time.strftime(\"%m-%d %H:%M:%S\") + \\\n",
    "\t\t\t\t\" [%2d] Lr: %5f, Training: %.2f%%, \"    %(epoch, lr, 100 * (num / loader.__len__())) + \\\n",
    "\t\t\t\t\" Loss: %.5f \\r\"        %(loss/(num)))\n",
    "\t\t\t\tsys.stderr.flush()\n",
    "\t\t\t\tsys.stdout.write(\"\\n\")\n",
    "\n",
    "\t\treturn loss/num, lr \n",
    "\n",
    "\tdef eval_network(self, loader):\n",
    "\t\tself.eval()\n",
    "\t\texp = []\n",
    "\t\treal_exp = []\n",
    "\t\tfor idx, (data,labels) in tqdm(enumerate(loader)):\n",
    "\t\t\tdata_1 = torch.FloatTensor(data).cuda()\n",
    "\t\t\twith torch.no_grad():\n",
    "\t\t\t\texpression, _ = self.regressor.forward(data_1)\n",
    "\t\t\tif len(expression.shape) > 1 :\n",
    "\t\t\t\texpression = expression.reshape(-1)\n",
    "\t\t\t\texpression = expression.detach().cpu().numpy()\n",
    "\t\t\t\tlabels = labels.detach().cpu().numpy() \n",
    "\t\t\texp.append(expression)\n",
    "\t\t\treal_exp.append(labels)\n",
    "\t\t\t\n",
    "\t\t# Coumpute Metric\n",
    "\t\texp = np.array(exp).reshape(-1)\n",
    "\t\treal_exp = np.array(real_exp).reshape(-1)\n",
    "\t\tPR = pearson_r(exp, real_exp)\n",
    "\n",
    "\t\treturn PR\n",
    "\n",
    "\tdef save_parameters(self, path):\n",
    "\t\ttorch.save(self.state_dict(), path)\n",
    "\n",
    "\tdef load_parameters(self, path):\n",
    "\t\tself_state = self.state_dict()\n",
    "\t\tloaded_state = torch.load(path)\n",
    "\t\tfor name, param in loaded_state.items():\n",
    "\t\t\torigname = name\n",
    "\t\t\tif name not in self_state:\n",
    "\t\t\t\tname = name.replace(\"module.\", \"\")\n",
    "\t\t\t\tif name not in self_state:\n",
    "\t\t\t\t\tprint(\"%s is not in the model.\"%origname)\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tif self_state[name].size() != loaded_state[origname].size():\n",
    "\t\t\t\tprint(\"Wrong parameter length: %s, model: %s, loaded: %s\"%(origname, self_state[name].size(), loaded_state[origname].size()))\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tself_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOtXgSxYvy0F"
   },
   "source": [
    "## Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGLEOzKhpPE2"
   },
   "source": [
    "### Epoch 1 \n",
    "\n",
    "initial epoch: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2VpPeGE-OeX-",
    "outputId": "0cf7da67-67f8-4b6d-b151-a0c481d9fcb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:1\n",
      "Current cuda device: 1\n",
      "number of unique sequences in the first 17 positions: 1\n",
      "number of unique sequences in the last 13 positions: 1\n",
      "(7111, 200)\n",
      "(7111, 200)\n",
      "(7111, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n",
      "(7110, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7110, 200)\n",
      "(71103, 200)\n",
      "(71103,)\n",
      "03-10 12:21:11 Model para number(백만) = 17.19\n",
      "LR :  1.200000000000002e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "200it [06:15,  1.90s/it]03-10 12:27:26 [ 1] Lr: 0.000012, Training: 1.60%,  Loss: 1.04838 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "232it [07:18,  1.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [37]\u001b[0m, in \u001b[0;36m<cell line: 51>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m score_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(score_save_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma+\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     50\u001b[0m \t\u001b[38;5;66;03m## Training for one epoch\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m \tloss, lr \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_network\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \t\u001b[38;5;66;03m## Evaluation every [test_step] epochs\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \t\u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36mRegressorModel.train_network\u001b[0;34m(self, epoch, loader)\u001b[0m\n\u001b[1;32m     47\u001b[0m lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_last_lr()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m index \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(labels)\n\u001b[0;32m---> 49\u001b[0m loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[1;32m     51\u001b[0m \tsys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(time\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     52\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m%2d\u001b[39;00m\u001b[38;5;124m] Lr: \u001b[39m\u001b[38;5;132;01m%5f\u001b[39;00m\u001b[38;5;124m, Training: \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;132;01m%%\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m    \u001b[38;5;241m%\u001b[39m(epoch, lr, \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m*\u001b[39m (num \u001b[38;5;241m/\u001b[39m loader\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__len__\u001b[39m())) \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m     53\u001b[0m \t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Loss: \u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m        \u001b[38;5;241m%\u001b[39m(loss\u001b[38;5;241m/\u001b[39m(num)))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is the main code of the ECAPATDNN project, to define the parameters and build the construction\n",
    "'''\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "import argparse, glob, os, torch, warnings, time\n",
    "\n",
    "\n",
    "model_save_path = \"exps/model_reproducing_3module\"\n",
    "score_save_path = \"exps/score_reproducing_3module.txt\"\n",
    "os.makedirs(model_save_path,exist_ok = True)\n",
    "\n",
    "device = ARGS['device']\n",
    "torch.cuda.set_device(device)\n",
    "print('Device:', device)\n",
    "print('Current cuda device:', torch.cuda.current_device())\n",
    "\n",
    "\n",
    "## Define the data loader\n",
    "trainloader = train_loader(train_data)\n",
    "trainLoader = torch.utils.data.DataLoader(trainloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "valloader = train_loader(val_data)\n",
    "valLoader = torch.utils.data.DataLoader(valloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "testloader = test_loader(ARGS)\n",
    "testLoader = torch.utils.data.DataLoader(testloader, batch_size = ARGS['batch_size'], shuffle = True, num_workers = 0, drop_last = True)\n",
    "\n",
    "## Search for the exist models\n",
    "modelfiles = glob.glob('%s/model_0*.model'%model_save_path)\n",
    "modelfiles.sort()\n",
    "\n",
    "## Otherwise, system will try to start from the saved model&epoch\n",
    "if len(modelfiles) >= 1:\n",
    "\tprint(\"Model %s loaded from previous state!\"%modelfiles[-1])\n",
    "\tepoch = int(os.path.splitext(os.path.basename(modelfiles[-1]))[0][6:]) + 1\n",
    "\ts = RegressorModel(ARGS)\n",
    "\ts.load_parameters(modelfiles[-1])\n",
    "\teval_pr_ = s.eval_network(testLoader)\n",
    "\tprint('Previous Eval Pearson_R : ',eval_pr_)\n",
    "## Otherwise, system will train from scratch\n",
    "else:\n",
    "\tepoch = 1\n",
    "\ts = RegressorModel(ARGS)\n",
    "\n",
    "pr = []\n",
    "eval_pr = []\n",
    "score_file = open(score_save_path, \"a+\")\n",
    "\n",
    "while(1):\n",
    "\t## Training for one epoch\n",
    "\tloss, lr = s.train_network(epoch = epoch, loader = trainLoader)\n",
    "\n",
    "\t## Evaluation every [test_step] epochs\n",
    "\tif epoch % 1 == 0:\n",
    "\t\ts.save_parameters(model_save_path + \"/model_%04d.model\"%epoch)\n",
    "\t\tpr.append(s.eval_network(valLoader))\n",
    "\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, Pearson_R %2.2f%%, bestPearson_R %2.2f%%\"%(epoch, pr[-1], max(pr)))\n",
    "\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, Pearson_R %2.2f%%, bestPearson_R %2.2f%%\\n\"%(epoch, lr, loss, pr[-1], max(pr)))\n",
    "\t\tscore_file.flush()\n",
    "\t\tif pr[-1] == max(pr) :\n",
    "\t\t\ts.save_parameters(model_save_path + \"/model_best.model\")\n",
    "\t\t\teval_pr.append(s.eval_network(testLoader))\n",
    "\t\t\tprint(time.strftime(\"%Y-%m-%d %H:%M:%S\"), \"%d epoch, Eval_Pearson_R %2.2f%%, Best_Eval_Pearson_R %2.2f%%\"%(epoch, eval_pr[-1], max(eval_pr)))\n",
    "\t\t\tscore_file.write(\"%d epoch, LR %f, LOSS %f, Pearson_R %2.2f%%, Best_Eval_Pearson_R %2.2f%%\\n\"%(epoch, lr, loss, eval_pr[-1], max(eval_pr)))\n",
    "\t\t\tscore_file.flush()\n",
    "\n",
    "\tif epoch >= ARGS['epochs']:\n",
    "\t\tquit()\n",
    "\n",
    "\tepoch += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zp22H950YtNt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19E3HT1ESJ4A"
   },
   "source": [
    "## Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "53wVoagb4tD8",
    "outputId": "74fa0970-dc30-492f-9cc9-76ce90418ec9"
   },
   "outputs": [],
   "source": [
    "session_info.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFWCf4x-5bTE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
